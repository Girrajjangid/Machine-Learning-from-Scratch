{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hessian matrix\n",
    "The **\"Hessian matrix\"** of a multivariable function f(x, y, z, ...) which different authors write as \n",
    "H(f) , Hf , or H ,  organizes all second partial derivatives into a matrix:\n",
    "$$\n",
    "\\mathbf{H} f=\\left[\\begin{array}{cccc}{\\frac{\\partial^{2} f}{\\partial x^{2}}} & {\\frac{\\partial^{2} f}{\\partial x \\partial y}} & {\\frac{\\partial^{2} f}{\\partial x \\partial z}} & {\\cdots} \\\\ {\\frac{\\partial^{2} f}{\\partial y \\partial x}} & {\\frac{\\partial^{2} f}{\\partial y^{2}}} & {\\frac{\\partial^{2} f}{\\partial y \\partial z}} & {\\cdots} \\\\ {\\frac{\\partial^{2} f}{\\partial z \\partial x}} & {\\frac{\\partial^{2} f}{\\partial z_{y}}} & {\\frac{\\partial^{2} f}{\\partial z^{2}}} & {\\cdots} \\\\ {\\vdots} & {\\vdots} & {\\vdots} & {\\ddots}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{H} f\\left(x_{0}, y_{0}, \\ldots\\right)=\\left[\\begin{array}{ccc}{\\frac{\\partial^{2} f}{\\partial x^{2}}\\left(x_{0}, y_{0}, \\ldots\\right)} & {\\frac{\\partial^{2} f}{\\partial x \\partial y}\\left(x_{0}, y_{0}, \\ldots\\right)} & {\\cdots} \\\\ {\\frac{\\partial^{2} f}{\\partial y \\partial x}\\left(x_{0}, y_{0}, \\ldots\\right)} & {\\frac{\\partial^{2} f}{\\partial y^{2}}\\left(x_{0}, y_{0}, \\ldots\\right)} & {\\ldots} \\\\ {\\vdots} & {\\vdots} & {\\ddots}\\end{array}\\right]\n",
    "$$\n",
    "$$\n",
    "\\mathbf{H}_{i, j}=\\frac{\\partial^{2} f}{\\partial x_{i} \\partial y_{j}}\n",
    "$$\n",
    "\n",
    "\n",
    "The Hessian matrix of a convex function is positive semi-definite. Refining this property allows us to test if a critical point x is a local maximum, local minimum, or a saddle point, as follows:\n",
    "\n",
    "If the Hessian is positive definite at x, then f attains an isolated local minimum at x. If the Hessian is negative definite at x, then f attains an isolated local maximum at x. If the Hessian has both positive and negative eigenvalues then x is a saddle point for f. Otherwise the test is inconclusive. This implies that at a local minimum the Hessian is positive-semi-definite, and at a local maximum the Hessian is negative semi-definite.\n",
    "\n",
    "Note that for positive semidefinite and negative semidefinite Hessians the test is inconclusive (a critical point where the Hessian is semidefinite but not definite may be a local extremum or a saddle point). However, more can be said from the point of view of Morse theory.\n",
    "\n",
    "The second derivative test for functions of one and two variables is simple. In one variable, the Hessian contains just one second derivative; if it is positive then x is a local minimum, and if it is negative then x is a local maximum; if it is zero then the test is inconclusive. In two variables, the determinant can be used, because the determinant is the product of the eigenvalues. If it is positive then the eigenvalues are both positive, or both negative. If it is negative then the two eigenvalues have different signs. If it is zero, then the second derivative test is inconclusive.\n",
    "\n",
    "\n",
    "The determinant of the above matrix is also sometimes referred to as the **Hessian**.\n",
    "\n",
    "\n",
    "The Hessian matrix is related to the Jacobian matrix by $\\mathbf{H}(f(\\mathbf{x}))=\\mathbf{J}(\\nabla f(\\mathbf{x}))^{\\mathrm{T}}$\n",
    "\n",
    "## Critical point\n",
    "\n",
    "If the gradient (the vector of the partial derivatives) of a function f is zero at some point x, then f has a critical point (or stationary point) at x. The determinant of the Hessian at x is then called the discriminant. If this determinant is zero then x is called a degenerate critical point of f, or a non-Morse critical point of f. Otherwise it is non-degenerate, and called a Morse critical point of f.\n",
    "\n",
    "# Lets take a example of computing Hessian matrix.\n",
    "\n",
    "**Problem:** Compute the Hessian of $f(x, y)=x^{3}-2 x y-y^{6}$ at the point (1,2):\n",
    "\n",
    "**Solution:** Ultimately we need all the second partial derivatives of f, so let's first compute both partial derivatives:\n",
    "$$\n",
    "\\begin{array}{c}{f_{x}(x, y)=\\frac{\\partial}{\\partial x}\\left(x^{3}-2 x y-y^{6}\\right)=3 x^{2}-2 y} \\\\ {f_{y}(x, y)=\\frac{\\partial}{\\partial y}\\left(x^{3}-2 x y-y^{6}\\right)=-2 x-6 y^{5}}\\end{array}\n",
    "$$\n",
    "\n",
    "With these, we compute all four second partial derivatives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} f_{x x}(x, y) &=\\frac{\\partial}{\\partial x}\\left(3 x^{2}-2 y\\right)=6 x \\\\ f_{x y}(x, y) &=\\frac{\\partial}{\\partial y}\\left(3 x^{2}-2 y\\right)=-2 \\\\ f_{y x}(x, y) &=\\frac{\\partial}{\\partial x}\\left(-2 x-6 y^{5}\\right)=-2 \\\\ f_{y y}(x, y) &=\\frac{\\partial}{\\partial y}\\left(-2 x-6 y^{5}\\right)=-30 y^{4} \\end{aligned}\n",
    "$$\n",
    "The Hessian matrix in this case is a 2×2 matrix with these functions as entries:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} f(x, y)=\\left[\\begin{array}{cc}{f_{x x}(x, y)} & {f_{x y}(x, y)} \\\\ {f_{y x}(x, y)} & {f_{y y}(x, y)}\\end{array}\\right]=\\left[\\begin{array}{cc}{6 x} & {-2} \\\\ {-2} & {-30 y^{4}}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "We were asked to evaluate this at the point $(x, y)=(1,2)$ so we plug in these values:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} f(1,2)=\\left[\\begin{array}{cc}{6(1)} & {-2} \\\\ {-2} & {-30(2)^{4}}\\end{array}\\right]=\\left[\\begin{array}{cc}{6} & {-2} \\\\ {-2} & {-480}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Now, the problem is ambiguous, since the \"Hessian\" can refer either to this matrix or to its determinant. What you want depends on context. For example, in optimizing multivariable functions, there is something called the \"second partial derivative test\" which uses the Hessian determinant. When the Hessian is used to approximate functions, you just use the matrix itself.\n",
    "\n",
    "If it's the determinant we want, here's what we get:\n",
    "\n",
    "$$\n",
    "\\operatorname{det}\\left(\\left[\\begin{array}{cc}{6} & {-2} \\\\ {-2} & {-480}\\end{array}\\right]\\right)=6(-480)-(-2)(-2)=-2884\n",
    "$$\n",
    "\n",
    "\n",
    "**Did you getting the point ??**\n",
    "\n",
    "In first phase we have single function with 2 parameters so we get two values.\n",
    "\n",
    "In second phase suppose the values which we get from first phase treat them as a two function with 2 parameters and compute \n",
    "their derivative with same terminology or concept\n",
    "\n",
    "# Newton's Raphson Method\n",
    "\n",
    "Newton's method is an iterative method for finding the roots of a differentiable function f, which are solutions to the equation f (x) = 0.\n",
    "\n",
    "Often Newton's method is modified to include a small step size γ ∈ (0,1) instead of γ = 1\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{n+1}=\\mathbf{x}_{n}-\\gamma\\left[\\mathbf{H} f\\left(\\mathbf{x}_{n}\\right)\\right]^{-1} \\nabla f\\left(\\mathbf{x}_{n}\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
