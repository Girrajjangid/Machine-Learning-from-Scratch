{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Regression\n",
    "\n",
    "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)∈{0,1}. We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle y(i)∈{1,…,K} where K is the number of classes.\n",
    "\n",
    "Recall that in logistic regression, we had a training set {(x(1),y(1)),…,(x(m),y(m))} of m labeled examples, where the input features are x(i)∈Rn. With logistic regression, we were in the binary classification setting, so the labels were y(i)∈{0,1}. Our hypothesis took the form:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}{h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta T_{x}}}} \\end{array}\n",
    "$$\n",
    "\n",
    "and the model parameters θ were trained to minimize the cost function\n",
    "\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{i} \\log \\left(h_{\\theta}\\left(x^{i}\\right)\\right)+\\left(1-y^{i}\\right) \\log \\left(1-h_{\\theta}\\left(x^{i}\\right)\\right)\\right]\n",
    "$$\n",
    "\n",
    "In the softmax regression setting, we are interested in multi-class classification (as opposed to only binary classification), and so the label y can take on K different values, rather than only two. Thus, in our training set {(x(1),y(1)),…,(x(m),y(m))}, we now have that y(i)∈{1,2,…,K}. (Note that our convention will be to index the classes starting from 1, rather than from 0.) For example, in the MNIST digit recognition task, we would have K=10 different classes.\n",
    "\n",
    "Given a test input x, we want our hypothesis to estimate the probability that P(y=k|x) for each value of k=1,…,K. I.e., we want to estimate the probability of the class label taking on each of the K different possible values. Thus, our hypothesis will output a K-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities. Concretely, our hypothesis hθ(x) takes the form:\n",
    "\n",
    "\n",
    "$$h_{\\theta}(x)=\\left[ \\begin{array}{c}{P(y=1 | x ; \\theta)} \\\\ {P(y=2 | x ; \\theta)} \\\\ {\\vdots} \\\\ {P(y=K | x ; \\theta)}\\end{array}\\right]=\\frac{1}{\\sum_{j=1}^{K} \\exp \\left(\\theta^{(j) \\top} x\\right)} \\left[ \\begin{array}{c}{\\exp \\left(\\theta^{(1) \\top} x\\right)} \\\\ {\\exp \\left(\\theta^{(2) \\top} x\\right)} \\\\ {\\vdots} \\\\ {\\exp \\left(\\theta^{(K) \\top} x\\right)}\\end{array}\\right]$$\n",
    "\n",
    "Here $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(K)} \\in \\mathfrak{R}^{n}$ are the parameters of our model. Notice that the term $\\frac{1}{\\sum_{j=1}^{K} \\exp \\left(\\theta^{(j) \\top} x\\right)}$ normalizes the distribution, so that it sums to one.\n",
    "\n",
    "# Cost Function\n",
    "\n",
    "We now describe the cost function that we’ll use for softmax regression.\n",
    "\n",
    "Our cost function will be\n",
    "$$J(\\theta)=-\\left[\\sum_{i=1}^{m} \\sum_{k=1}^{K} 1\\left\\{y^{(i)}=k\\right\\} \\log \\frac{\\exp \\left(\\theta^{(k) \\top} x^{(i)}\\right)}{\\sum_{j=1}^{K} \\exp \\left(\\theta^{(j) \\top} x^{(i)}\\right)}\\right]$$\n",
    "\n",
    "Notice that this generalizes the logistic regression cost function, which could also have been written:\n",
    "\n",
    "$$\\begin{aligned} J(\\theta) &=-\\left[\\sum_{i=1}^{m}\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)+y^{(i)} \\log h_{\\theta}\\left(x^{(i)}\\right)\\right] \\\\ &=-\\left[\\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y^{(i)}=k\\right\\} \\log P\\left(y^{(i)}=k | x^{(i)} ; \\theta\\right)\\right] \\end{aligned}$$\n",
    "\n",
    "The softmax cost function is similar, except that we now sum over the K different possible values of the class label. Note also that in softmax regression, we have that\n",
    "\n",
    "$$P\\left(y^{(i)}=k | x^{(i)} ; \\theta\\right)=\\frac{\\exp \\left(\\theta^{(k) \\top} x^{(i)}\\right)}{\\sum_{j=1}^{K} \\exp \\left(\\theta^{(j) \\top} x^{(i)}\\right)}$$\n",
    "\n",
    "\n",
    "We cannot solve for the minimum of J(θ) analytically, and thus as usual we’ll resort to an iterative optimization algorithm. Taking derivatives, one can show that the gradient is:\n",
    "\n",
    "$$\\nabla_{\\theta^{(k)}} J(\\theta)=-\\sum_{i=1}^{m}\\left[x^{(i)}\\left(1\\left\\{y^{(i)}=k\\right\\}-P\\left(y^{(i)}=k | x^{(i)} ; \\theta\\right)\\right)\\right]$$\n",
    "\n",
    "Recall the meaning of the ”$\\nabla_{\\theta^{(k)}}$” notation. In particular, $\\nabla_{\\theta^{(k)}} J(\\theta)$ is itself a vector, so that its j-th element is $\\frac{\\partial J(\\theta)}{\\partial \\theta_{l k}}$ the partial derivative of $J(\\theta)$ with respect to the j-th element of $\\theta(k)$.\n",
    "\n",
    "Armed with this formula for the derivative, one can then plug it into a standard optimization package and have it minimize $J(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('glass.csv')\n",
    "N_labels = len(set(raw_data['Type']))\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null entries\n",
    "sns.heatmap(raw_data.isnull(),cbar=False)\n",
    "# no entry is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in raw_data:\n",
    "    sns.distplot(raw_data[feat])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These graphs clearly shows that our data in not Normally distributed it means due to some features our classification algorithm may be disturb\n",
    "\n",
    "## Lets find the correlation between all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = raw_data.columns[:-1].tolist()\n",
    "corr = raw_data[features].corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
    "           xticklabels= features, yticklabels= features, cmap= 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ca and RI are positivity correlated i.e both are dependent Ba and Al also\n",
    "## Therefore \n",
    "## 1. Logistic regression may not give us perfect results because according to there assumptions all independent variables are independent to each other.\n",
    "## 2. Maximum liklihood not give us good results in small data sets\n",
    "\n",
    "## But we try to compute classification on this data without any further changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(raw_data['Type'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have 6 labels = [1,2,3,5,6,7] out of which 1 and 2 label contains more than 70 % of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data seperation\n",
    "z = list(map(lambda i:raw_data[raw_data['Type']==i].iloc[:int(0.7*raw_data[raw_data['Type']==i].shape[0]),:],\n",
    "             set(raw_data['Type'])))\n",
    "training_data = pd.DataFrame()\n",
    "training_data = training_data.append(z)\n",
    "training_data_labels = np.array(training_data['Type'])\n",
    "training_data.drop('Type',inplace=True,axis=1)\n",
    "training_data = np.array(training_data)\n",
    "training_data = (training_data)/np.max(training_data)  # normalized data\n",
    "\n",
    "x = list(map(lambda i:raw_data[raw_data['Type']==i].iloc[:int(0.3*raw_data[raw_data['Type']==i].shape[0]),:],set(raw_data['Type'])))\n",
    "testing_data = pd.DataFrame()\n",
    "testing_data = testing_data.append(x)\n",
    "testing_data_labels = np.array(testing_data['Type'])\n",
    "testing_data.drop('Type',inplace=True,axis=1)\n",
    "testing_data = np.array(testing_data)\n",
    "testing_data = (testing_data)/np.max(testing_data)  # normalized data\n",
    "\n",
    "pd.DataFrame(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "A one hot encoding is a representation of categorical variables as binary vectors.\n",
    "\n",
    "This first requires that the categorical values be mapped to integer values.\n",
    "\n",
    "Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(label):\n",
    "    s = pd.Series(label)\n",
    "    return np.array(pd.get_dummies(s))\n",
    "training_label_hot_encoded = oneHotEncoding(training_data_labels)\n",
    "testing_label_hot_encoded = oneHotEncoding(testing_data_labels)\n",
    "training_label_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(theta0 , theta , data):\n",
    "    exp = np.exp(theta0 + np.dot(data , theta))\n",
    "    sum_denominator = np.sum((exp),axis=1)\n",
    "    s = list(map(lambda i : exp[:,i] / sum_denominator , np.arange(N_labels)))\n",
    "    return np.array(s).T\n",
    "\n",
    "alpha = 0.001\n",
    "epsilon = 0.00001\n",
    "initial_theta0 = np.random.randn(1,N_labels) \n",
    "initial_theta = np.random.randn(training_data.shape[1] , N_labels)\n",
    "i = 0\n",
    "fc = []\n",
    "itera = []\n",
    "while(True):\n",
    "    i+=1\n",
    " \n",
    "    del_theta0 = (np.sum((softmax(initial_theta0 , initial_theta , training_data) - training_label_hot_encoded),axis = 0))/training_data.shape[0]\n",
    "    del_theta = (np.matmul(training_data.T,(softmax(initial_theta0,initial_theta,training_data)-training_label_hot_encoded)))/training_data.shape[0]\n",
    "\n",
    "    final_theta0 = initial_theta0 - alpha * del_theta0\n",
    "    final_theta = initial_theta - alpha * del_theta\n",
    "\n",
    "    initial_cost = (-np.sum(\n",
    "        np.matmul(np.log(softmax(initial_theta0 , initial_theta , training_data)).T , training_label_hot_encoded )\n",
    "        ))/training_data.shape[0]\n",
    "\n",
    "    final_cost = (-np.sum(\n",
    "        np.matmul(np.log(softmax(final_theta0 , final_theta , training_data)).T , training_label_hot_encoded )\n",
    "        ))/training_data.shape[0]\n",
    "    \n",
    "    if (abs(final_cost - initial_cost)) < epsilon:\n",
    "        break\n",
    "    print('at {} iteration ANL : {}'.format(i,initial_cost))\n",
    "    initial_theta0 = final_theta0\n",
    "    initial_theta  = final_theta\n",
    "    itera.append(i)\n",
    "    fc.append(final_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(itera , fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curve going to decrease as we increase iterations i.e it's a sign of that gradient decent works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
