{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Minimal Optimization\n",
    "\n",
    "we will use an algorithm specifically created to solve this problem quickly: the SMO (sequential minimal optimization) algorithm. Most machine learning libraries use the SMO algorithm or some variation. \n",
    "\n",
    "The SMO algorithm will solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{minimize}}} & {\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)-\\sum_{i=1}^{m} \\alpha_{i}} \\\\ {\\text { subject to }} & {0 \\leq \\alpha_{i} \\leq C, \\text { for any } i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "\n",
    "It is a kernelized version of the soft-margin formulation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T) \n",
    " \n",
    "def objective_function_to_minimize(X, y, a, kernel):\n",
    "    m, n = np.shape(X)\n",
    "    cal = (1 / 2) * np.sum( [a[i] * a[j] * y[i] * y[j] * kernel(X[i, :], X[j, :]) for j in range(m) for i in range(m)] ) \\\n",
    "                        - np.sum(a)\n",
    "    return cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T) \n",
    " \n",
    "def objective_function_to_minimize(X, y, a, kernel):\n",
    "    m, n = np.shape(X)\n",
    "    return 1 / 2 * np.sum([a[i] * a[j] * y[i] * y[j]* kernel(X[i, :], X[j, :])\n",
    "                           for j in range(m)\n",
    "                           for i in range(m)])\\\n",
    "                - np.sum([a[i] for i in range(m)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same problem we solved using CVXOPT. Why do we need another method? Because we would like to be able to use SVMs with big datasets, and using convex optimization packages usually involves matrix operations that take a lot of time as the size of the matrix increases or become impossible because of memory limitations. The SMO algorithm has been created with the goal of being faster than other methods\n",
    "\n",
    "# The idea behind SMO \n",
    "\n",
    "The idea behind SMO is quite easy: we will solve a simpler problem. That is, given a vector $\\alpha=\\left(\\alpha_{1}, \\alpha_{2}, \\ldots, \\alpha_{m}\\right)$, we will allow ourselves to change only two values of $\\alpha$, for instance, $\\alpha_3$ and $\\alpha_7$. We will change them until the objective function reaches its minimum given this set of alphas. Then we will pick two other alphas and change them until the function returns its smallest value, and so on. If we continue doing that, we will eventually reach the minimum of the objective function of the original problem. \n",
    "SMO solves a sequence of several simpler optimization problems. \n",
    "\n",
    "### The SMO algorithm is composed of three parts: \n",
    "\n",
    "• One heuristic to choose the first Lagrange multiplier \n",
    "\n",
    "• One heuristic to choose the second Lagrange multiplier \n",
    "\n",
    "• The code to solve the optimization problem analytically for the two chosen multipliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_bound_indexes(self):\n",
    "    return np.where(np.logical_and(self.alphas > 0,\n",
    "                                   self.alphas < self.C))[0] \n",
    " \n",
    "# First heuristic: loop  over examples where alpha is not 0 and not C \n",
    "# they are the most likely to violate the KKT conditions \n",
    "# (the non-bound subset). \n",
    "def first_heuristic(self):\n",
    "    num_changed = 0\n",
    "    non_bound_idx = self.get_non_bound_indexes() \n",
    "    for i in non_bound_idx:\n",
    "        num_changed += self.examine_example(i)\n",
    "        return num_changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_routine(self):\n",
    "    num_changed = 0\n",
    "    examine_all = True \n",
    " \n",
    "    while num_changed > 0 or examine_all:\n",
    "        num_changed = 0 \n",
    "        if examine_all:\n",
    "            for i in range(self.m):\n",
    "                num_changed += self.examine_example(i)\n",
    "        else:\n",
    "            num_changed += self.first_heuristic() \n",
    " \n",
    "        if examine_all:\n",
    "            examine_all = False\n",
    "        elif num_changed == 0:\n",
    "            examine_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_heuristic(self, non_bound_indices):\n",
    "    i1 = -1\n",
    "    if len(non_bound_indices) > 1:\n",
    "        maxi = 0 \n",
    "    for j in non_bound_indices:\n",
    "        E1 = self.errors[j] - self.y[j]\n",
    "        step = abs(E1 - self.E2)# approximation\n",
    "        if step > max:\n",
    "            maxi = step\n",
    "            i1 = j\n",
    "    return i1 \n",
    " \n",
    "\n",
    "def examine_example(self, i2):\n",
    "    self.y2 = self.y[i2]\n",
    "    self.a2 = self.alphas[i2]\n",
    "    self.X2 = self.X[i2]\n",
    "    self.E2 = self.get_error(i2) \n",
    " \n",
    "    r2 = self.E2 * self.y2 \n",
    " \n",
    "    if not((r2 < -self.tol and self.a2 < self.C) or (r2 > self.tol and self.a2 > 0)):\n",
    "        # The KKT conditions are met, SMO looks at another example.\n",
    "        return 0 \n",
    " \n",
    "    # Second heuristic A: choose the Lagrange multiplier that     \n",
    "    # maximizes the absolute error.\n",
    "    non_bound_idx = list(self.get_non_bound_indexes())\n",
    "    i1 = self.second_heuristic(non_bound_idx) \n",
    " \n",
    "    if i1 >= 0 and self.take_step(i1, i2):\n",
    "        return 1 \n",
    " \n",
    "    # Second heuristic B: Look for examples making positive\n",
    "    # progress by looping over all non-zero and non-C alpha,\n",
    "    # starting at a random point.\n",
    "    if len(non_bound_idx) > 0:\n",
    "        rand_i = randrange(len(non_bound_idx))\n",
    "        for i1 in (non_bound_idx[rand_i:] + non_bound_idx[:rand_i]):\n",
    "            if self.take_step(i1, i2):\n",
    "                return 1 \n",
    " \n",
    "    # Second heuristic C: Look for examples making positive progress\n",
    "    # by looping over all possible examples, starting at a random \n",
    "    # point.\n",
    "    rand_i = randrange(self.m)\n",
    "    all_indices = list(range(self.m))\n",
    "    for i1 in all_indices[rand_i:] + all_indices[:rand_i]:\n",
    "        if self.take_step(i1, i2):\n",
    "            return 1 \n",
    " \n",
    "    # Extremely degenerate circumstances, SMO skips the first example.     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
