{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Minimal Optimization\n",
    "\n",
    "we will use an algorithm specifically created to solve this problem quickly: the SMO (sequential minimal optimization) algorithm. Most machine learning libraries use the SMO algorithm or some variation. \n",
    "\n",
    "The SMO algorithm will solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{minimize}}} & {\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)-\\sum_{i=1}^{m} \\alpha_{i}} \\\\ {\\text { subject to }} & {0 \\leq \\alpha_{i} \\leq C, \\text { for any } i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "\n",
    "It is a kernelized version of the soft-margin formulation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T) \n",
    " \n",
    "def objective_function_to_minimize(X, y, a, kernel):\n",
    "    m, n = np.shape(X)\n",
    "    cal = (1 / 2) * np.sum( [a[i] * a[j] * y[i] * y[j] * kernel(X[i, :], X[j, :]) for j in range(m) for i in range(m)] ) \\\n",
    "                        - np.sum(a)\n",
    "    return cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x1, x2):\n",
    "    return np.dot(x1, x2.T) \n",
    " \n",
    "def objective_function_to_minimize(X, y, a, kernel):\n",
    "    m, n = np.shape(X)\n",
    "    return 1 / 2 * np.sum([a[i] * a[j] * y[i] * y[j]* kernel(X[i, :], X[j, :])\n",
    "                           for j in range(m)\n",
    "                           for i in range(m)])\\\n",
    "                - np.sum([a[i] for i in range(m)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same problem we solved using CVXOPT. Why do we need another method? Because we would like to be able to use SVMs with big datasets, and using convex optimization packages usually involves matrix operations that take a lot of time as the size of the matrix increases or become impossible because of memory limitations. The SMO algorithm has been created with the goal of being faster than other methods\n",
    "\n",
    "# The idea behind SMO \n",
    "\n",
    "The idea behind SMO is quite easy: we will solve a simpler problem. That is, given a vector $\\alpha=\\left(\\alpha_{1}, \\alpha_{2}, \\ldots, \\alpha_{m}\\right)$, we will allow ourselves to change only two values of $\\alpha$, for instance, $\\alpha_3$ and $\\alpha_7$. We will change them until the objective function reaches its minimum given this set of alphas. Then we will pick two other alphas and change them until the function returns its smallest value, and so on. If we continue doing that, we will eventually reach the minimum of the objective function of the original problem. \n",
    "SMO solves a sequence of several simpler optimization problems. \n",
    "\n",
    "### The SMO algorithm is composed of three parts: \n",
    "\n",
    "• One heuristic to choose the first Lagrange multiplier \n",
    "\n",
    "• One heuristic to choose the second Lagrange multiplier \n",
    "\n",
    "• The code to solve the optimization problem analytically for the two chosen multipliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_bound_indexes(self):\n",
    "    return np.where(np.logical_and(self.alphas > 0,\n",
    "                                   self.alphas < self.C))[0] \n",
    " \n",
    "# First heuristic: loop  over examples where alpha is not 0 and not C \n",
    "# they are the most likely to violate the KKT conditions \n",
    "# (the non-bound subset). \n",
    "def first_heuristic(self):\n",
    "    num_changed = 0\n",
    "    non_bound_idx = self.get_non_bound_indexes() \n",
    "    for i in non_bound_idx:\n",
    "        num_changed += self.examine_example(i)\n",
    "        return num_changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_routine(self):\n",
    "    num_changed = 0\n",
    "    examine_all = True \n",
    " \n",
    "    while num_changed > 0 or examine_all:\n",
    "        num_changed = 0 \n",
    "        if examine_all:\n",
    "            for i in range(self.m):\n",
    "                num_changed += self.examine_example(i)\n",
    "        else:\n",
    "            num_changed += self.first_heuristic() \n",
    " \n",
    "        if examine_all:\n",
    "            examine_all = False\n",
    "        elif num_changed == 0:\n",
    "            examine_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_heuristic(self, non_bound_indices):\n",
    "    i1 = -1\n",
    "    if len(non_bound_indices) > 1:\n",
    "        maxi = 0 \n",
    "    for j in non_bound_indices:\n",
    "        E1 = self.errors[j] - self.y[j]\n",
    "        step = abs(E1 - self.E2)# approximation\n",
    "        if step > max:\n",
    "            maxi = step\n",
    "            i1 = j\n",
    "    return i1 \n",
    " \n",
    "\n",
    "def examine_example(self, i2):\n",
    "    self.y2 = self.y[i2]\n",
    "    self.a2 = self.alphas[i2]\n",
    "    self.X2 = self.X[i2]\n",
    "    self.E2 = self.get_error(i2) \n",
    " \n",
    "    r2 = self.E2 * self.y2 \n",
    " \n",
    "    if not((r2 < -self.tol and self.a2 < self.C) or (r2 > self.tol and self.a2 > 0)):\n",
    "        # The KKT conditions are met, SMO looks at another example.\n",
    "        return 0 \n",
    " \n",
    "    # Second heuristic A: choose the Lagrange multiplier that     \n",
    "    # maximizes the absolute error.\n",
    "    non_bound_idx = list(self.get_non_bound_indexes())\n",
    "    i1 = self.second_heuristic(non_bound_idx) \n",
    " \n",
    "    if i1 >= 0 and self.take_step(i1, i2):\n",
    "        return 1 \n",
    " \n",
    "    # Second heuristic B: Look for examples making positive\n",
    "    # progress by looping over all non-zero and non-C alpha,\n",
    "    # starting at a random point.\n",
    "    if len(non_bound_idx) > 0:\n",
    "        rand_i = randrange(len(non_bound_idx))\n",
    "        for i1 in (non_bound_idx[rand_i:] + non_bound_idx[:rand_i]):\n",
    "            if self.take_step(i1, i2):\n",
    "                return 1 \n",
    " \n",
    "    # Second heuristic C: Look for examples making positive progress\n",
    "    # by looping over all possible examples, starting at a random \n",
    "    # point.\n",
    "    rand_i = randrange(self.m)\n",
    "    all_indices = list(range(self.m))\n",
    "    for i1 in all_indices[rand_i:] + all_indices[:rand_i]:\n",
    "        if self.take_step(i1, i2):\n",
    "            return 1 \n",
    " \n",
    "    # Extremely degenerate circumstances, SMO skips the first example.     return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "# Written from the pseudo-code in :\n",
    "# http://luthuli.cs.uiuc.edu/~daf/courses/optimization/Papers/smoTR.pdf\n",
    "class SmoAlgorithm:\n",
    "    def __init__(self, X, y, C, tol, kernel, use_linear_optim):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.m, self.n = np.shape(self.X)\n",
    "        self.alphas = np.zeros(self.m)\n",
    "\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "\n",
    "        self.errors = np.zeros(self.m)\n",
    "        self.eps = 1e-3  # epsilon\n",
    "\n",
    "        self.b = 0\n",
    "\n",
    "        self.w = np.zeros(self.n)\n",
    "        self.use_linear_optim = use_linear_optim\n",
    "\n",
    "    # Compute the SVM output for example i\n",
    "    # Note that Platt uses the convention w.x-b=0\n",
    "    # while we have been using w.x+b in the book.\n",
    "    def output(self, i):\n",
    "        if self.use_linear_optim:\n",
    "            # Equation 1\n",
    "            return float(np.dot(self.w.T, self.X[i])) - self.b\n",
    "        else:\n",
    "            # Equation 10\n",
    "            return np.sum([self.alphas[j] * self.y[j]\n",
    "                           * self.kernel(self.X[j], self.X[i])\n",
    "                           for j in range(self.m)]) - self.b\n",
    "\n",
    "    # Try to solve the problem analytically.\n",
    "    def take_step(self, i1, i2):\n",
    "\n",
    "        if i1 == i2:\n",
    "            return False\n",
    "\n",
    "        a1 = self.alphas[i1]\n",
    "        y1 = self.y[i1]\n",
    "        X1 = self.X[i1]\n",
    "        E1 = self.get_error(i1)\n",
    "\n",
    "        s = y1 * self.y2\n",
    "\n",
    "        # Compute the bounds of the new alpha2.\n",
    "        if y1 != self.y2:\n",
    "            # Equation 13\n",
    "            L = max(0, self.a2 - a1)\n",
    "            H = min(self.C, self.C + self.a2 - a1)\n",
    "        else:\n",
    "            # Equation 14\n",
    "            L = max(0, self.a2 + a1 - self.C)\n",
    "            H = min(self.C, self.a2 + a1)\n",
    "\n",
    "        if L == H:\n",
    "            return False\n",
    "\n",
    "        k11 = self.kernel(X1, X1)\n",
    "        k12 = self.kernel(X1, self.X[i2])\n",
    "        k22 = self.kernel(self.X[i2], self.X[i2])\n",
    "\n",
    "        # Compute the second derivative of the\n",
    "        # objective function along the diagonal.\n",
    "        # Equation 15\n",
    "        eta = k11 + k22 - 2 * k12\n",
    "\n",
    "        if eta > 0:\n",
    "            # Equation 16\n",
    "            a2_new = self.a2 + self.y2 * (E1 - self.E2) / eta\n",
    "\n",
    "            # Clip the new alpha so that is stays at the end of the line.\n",
    "            # Equation 17\n",
    "            if a2_new < L:\n",
    "                a2_new = L\n",
    "            elif a2_new > H:\n",
    "                a2_new = H\n",
    "        else:\n",
    "            # Under unusual cicumstances, eta will not be positive.\n",
    "            # Equation 19\n",
    "            f1 = y1 * (E1 + self.b) - a1 * k11 - s * self.a2 * k12\n",
    "            f2 = self.y2 * (self.E2 + self.b) - s * a1 * k12 \\\n",
    "                 - self.a2 * k22\n",
    "            L1 = a1 + s(self.a2 - L)\n",
    "            H1 = a1 + s * (self.a2 - H)\n",
    "            Lobj = L1 * f1 + L * f2 + 0.5 * (L1 ** 2) * k11 \\\n",
    "                   + 0.5 * (L ** 2) * k22 + s * L * L1 * k12\n",
    "            Hobj = H1 * f1 + H * f2 + 0.5 * (H1 ** 2) * k11 \\\n",
    "                   + 0.5 * (H ** 2) * k22 + s * H * H1 * k12\n",
    "\n",
    "            if Lobj < Hobj - self.eps:\n",
    "                a2_new = L\n",
    "            elif Lobj > Hobj + self.eps:\n",
    "                a2_new = H\n",
    "            else:\n",
    "                a2_new = self.a2\n",
    "\n",
    "        # If alpha2 did not change enough the algorithm\n",
    "        # returns without updating the multipliers.\n",
    "        if abs(a2_new - self.a2) < self.eps * (a2_new + self.a2 \\\n",
    "                                                       + self.eps):\n",
    "            return False\n",
    "\n",
    "        # Equation 18\n",
    "        a1_new = a1 + s * (self.a2 - a2_new)\n",
    "\n",
    "        new_b = self.compute_b(E1, a1, a1_new, a2_new, k11, k12, k22, y1)\n",
    "\n",
    "        delta_b = new_b - self.b\n",
    "\n",
    "        self.b = new_b\n",
    "\n",
    "        # Equation 22\n",
    "        if self.use_linear_optim:\n",
    "            self.w = self.w + y1 * (a1_new - a1) * X1 \\\n",
    "                     + self.y2 * (a2_new - self.a2) * self.X2\n",
    "\n",
    "        # Update the error cache using the new Lagrange multipliers.\n",
    "        delta1 = y1 * (a1_new - a1)\n",
    "        delta2 = self.y2 * (a2_new - self.a2)\n",
    "\n",
    "        # Update the error cache.\n",
    "        for i in range(self.m):\n",
    "            if 0 < self.alphas[i] < self.C:\n",
    "                self.errors[i] += delta1 * self.kernel(X1, self.X[i]) + \\\n",
    "                                  delta2 * self.kernel(self.X2, self.X[i]) \\\n",
    "                                  - delta_b\n",
    "\n",
    "        self.errors[i1] = 0\n",
    "        self.errors[i2] = 0\n",
    "\n",
    "        self.alphas[i1] = a1_new\n",
    "        self.alphas[i2] = a2_new\n",
    "\n",
    "        return True\n",
    "\n",
    "    def compute_b(self, E1, a1, a1_new, a2_new, k11, k12, k22, y1):\n",
    "        # Equation 20\n",
    "        b1 = E1 + y1 * (a1_new - a1) * k11 + \\\n",
    "             self.y2 * (a2_new - self.a2) * k12 + self.b\n",
    "\n",
    "        # Equation 21\n",
    "        b2 = self.E2 + y1 * (a1_new - a1) * k12 + \\\n",
    "             self.y2 * (a2_new - self.a2) * k22 + self.b\n",
    "\n",
    "        if (0 < a1_new) and (self.C > a1_new):\n",
    "            new_b = b1\n",
    "        elif (0 < a2_new) and (self.C > a2_new):\n",
    "            new_b = b2\n",
    "        else:\n",
    "            new_b = (b1 + b2) / 2.0\n",
    "        return new_b\n",
    "\n",
    "    def get_error(self, i1):\n",
    "        if 0 < self.alphas[i1] < self.C:\n",
    "            return self.errors[i1]\n",
    "        else:\n",
    "            return self.output(i1) - self.y[i1]\n",
    "\n",
    "    def second_heuristic(self, non_bound_indices):\n",
    "        i1 = -1\n",
    "        if len(non_bound_indices) > 1:\n",
    "            max = 0\n",
    "\n",
    "            for j in non_bound_indices:\n",
    "                E1 = self.errors[j] - self.y[j]\n",
    "                step = abs(E1 - self.E2)  # approximation\n",
    "                if step > max:\n",
    "                    max = step\n",
    "                    i1 = j\n",
    "        return i1\n",
    "\n",
    "    def examine_example(self, i2):\n",
    "        self.y2 = self.y[i2]\n",
    "        self.a2 = self.alphas[i2]\n",
    "        self.X2 = self.X[i2]\n",
    "        self.E2 = self.get_error(i2)\n",
    "\n",
    "        r2 = self.E2 * self.y2\n",
    "\n",
    "        if not ((r2 < -self.tol and self.a2 < self.C) or\n",
    "                    (r2 > self.tol and self.a2 > 0)):\n",
    "            # The KKT conditions are met, SMO looks at another example.\n",
    "            return 0\n",
    "\n",
    "        # Second heuristic A: choose the Lagrange multiplier which\n",
    "        # maximizes the absolute error.\n",
    "        non_bound_idx = list(self.get_non_bound_indexes())\n",
    "        i1 = self.second_heuristic(non_bound_idx)\n",
    "\n",
    "        if i1 >= 0 and self.take_step(i1, i2):\n",
    "            return 1\n",
    "\n",
    "        # Second heuristic B: Look for examples making positive\n",
    "        # progress by looping over all non-zero and non-C alpha,\n",
    "        # starting at a random point.\n",
    "        if len(non_bound_idx) > 0:\n",
    "            rand_i = randrange(len(non_bound_idx))\n",
    "            for i1 in non_bound_idx[rand_i:] + non_bound_idx[:rand_i]:\n",
    "                if self.take_step(i1, i2):\n",
    "                    return 1\n",
    "\n",
    "        # Second heuristic C: Look for examples making positive progress\n",
    "        # by looping over all possible examples, starting at a random\n",
    "        # point.\n",
    "        rand_i = randrange(self.m)\n",
    "        all_indices = list(range(self.m))\n",
    "        for i1 in all_indices[rand_i:] + all_indices[:rand_i]:\n",
    "            if self.take_step(i1, i2):\n",
    "                return 1\n",
    "\n",
    "        # Extremely degenerate circumstances, SMO skips the first example.\n",
    "        return 0\n",
    "\n",
    "    def error(self, i2):\n",
    "        return self.output(i2) - self.y2\n",
    "\n",
    "    def get_non_bound_indexes(self):\n",
    "        return np.where(np.logical_and(self.alphas > 0,\n",
    "                                       self.alphas < self.C))[0]\n",
    "\n",
    "    # First heuristic: loop  over examples where alpha is not 0 and not C\n",
    "    # they are the most likely to violate the KKT conditions\n",
    "    # (the non-bound subset).\n",
    "    def first_heuristic(self):\n",
    "        num_changed = 0\n",
    "        non_bound_idx = self.get_non_bound_indexes()\n",
    "\n",
    "        for i in non_bound_idx:\n",
    "            num_changed += self.examine_example(i)\n",
    "        return num_changed\n",
    "\n",
    "    def main_routine(self):\n",
    "        num_changed = 0\n",
    "        examine_all = True\n",
    "\n",
    "        while num_changed > 0 or examine_all:\n",
    "            num_changed = 0\n",
    "\n",
    "            if examine_all:\n",
    "                for i in range(self.m):\n",
    "                    num_changed += self.examine_example(i)\n",
    "            else:\n",
    "                num_changed += self.first_heuristic()\n",
    "\n",
    "            if examine_all:\n",
    "                examine_all = False\n",
    "            elif num_changed == 0:\n",
    "                examine_all = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
