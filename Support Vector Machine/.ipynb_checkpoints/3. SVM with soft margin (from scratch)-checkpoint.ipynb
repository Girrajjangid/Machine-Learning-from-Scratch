{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft margin\n",
    " The goal is now not to make zero classification mistakes, but to make as few mistakes as possible.  \n",
    "\n",
    "To do so, they modified the constraints of the optimization problem by adding a variable $\\zeta$ (zeta). So the constraint\n",
    "\n",
    "$${y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1} $$\n",
    "\n",
    "becomes\n",
    "\n",
    "$${y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i}}$$\n",
    "\n",
    "As a result, when minimizing the objective function, it is possible to satisfy the constraint even if the example does not meet the original constraint (that is, it is too close from the hyperplane, or it is not on the correct side of the hyperplane)\n",
    "\n",
    "The problem with hard margin is that we cannot seperate non-seperable data. But soft margin allow us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    " \n",
    "w = np.array([0.4, 1]) \n",
    "b = -10 \n",
    " \n",
    "x = np.array([6, 8]) \n",
    "y = -1 \n",
    " \n",
    "def constraint(w, b, x, y):\n",
    "    return y * (np.dot(w, x) + b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def hard_constraint_is_satisfied(w, b, x, y):\n",
    "    return constraint(w, b, x, y) >= 1 \n",
    " \n",
    "def soft_constraint_is_satisfied(w, b, x, y, zeta):\n",
    "    return constraint(w, b, x, y) >= 1 - zeta \n",
    " \n",
    "    \n",
    "# While the constraint is not satisfied for the example (6,8). \n",
    "print(hard_constraint_is_satisfied(w, b, x, y))               # False \n",
    " \n",
    "# We can use zeta = 2 and satisfy the soft constraint. \n",
    "print(soft_constraint_is_satisfied(w, b, x, y, zeta=2))       # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# We can pick a huge zeta for every point \n",
    "# to always satisfy the soft constraint. \n",
    "print(soft_constraint_is_satisfied(w, b, x, y, zeta=10))   # True \n",
    "print(soft_constraint_is_satisfied(w, b, x, y, zeta=1000)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, we need to modify the objective function to penalize the choice of a big $\\zeta$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}{\\underset{\\mathbf{w}, b, \\zeta}{\\operatorname{minimize}}} & {\\frac{1}{2}\\|\\mathbf{w}\\|^{2}+\\sum_{i=1}^{m} \\zeta_{i}} \\\\ {\\text { subject to }} & {y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad \\text { for any } i=1, \\ldots, m}\\end{array}\n",
    "$$\n",
    "\n",
    "We take the sum of all individual $\\zeta_{i}$ and add it to the objective function. Adding such a penalty is called **regularization**. As a result, the solution will be the hyperplane that maximizes the margin while having the smallest error possible\n",
    "\n",
    "There is still a little problem. With this formulation, one can easily minimize the function by using negative values of $\\zeta_{i}$. We add the constraint $\\zeta_{i}>=0$ to prevent this. Moreover, we would like to keep some control over the soft margin. Maybe sometimes we want to use the hard marginâ€” after all, that is why we add the parameter 'C', which will help us to determine how important the $\\zeta$ should be (more on that later). \n",
    "\n",
    "This leads us to the **soft margin formulation**\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\mathbf{w}, b, \\zeta}{\\operatorname{minimize}}} & {\\frac{1}{2}\\|\\mathbf{w}\\|^{2}+C \\sum_{i=1}^{m} \\zeta_{i}} \\\\ {\\text { subject to }} & {y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i}} \\\\ {} & {\\zeta_{i} \\geq 0 \\quad \\text { for any } i=1, \\ldots, m}\\end{array}\n",
    "$$\n",
    "\n",
    "**The wolfe dual problem:**\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{maximize}}} & {\\sum_{i=1}^{m} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}} \\\\ {\\text { subject to }} & {0 \\leq \\alpha_{i} \\leq C, \\text { for any } i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
