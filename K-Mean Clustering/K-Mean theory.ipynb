{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector quantization (VQ)** is a classical quantization technique from **signal processing** that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-means** clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\n",
    "\n",
    "The problem is computationally difficult **(NP-hard)** efficient **heuristic algorithms** converge quickly to a local optimum.\n",
    "\n",
    "These are usually similar to the **expectation-maximization algorithm** for mixtures of Gaussian distributions via an iterative refinement approach employed by both **k-means** and **Gaussian mixture modeling.** They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\n",
    "\n",
    "The algorithm has a loose relationship to the **k-nearest neighbor classifier**, a popular machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as **nearest centroid classifier** or **Rocchio algorithm.**\n",
    "\n",
    "The term \"k-means\" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, though it wasn't published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. \n",
    "The results of the K-means clustering algorithm are:\n",
    "\n",
    "1. The centroids of the K clusters, which can be used to label new data\n",
    "2. Labels for the training data (each data point is assigned to a single cluster)\n",
    "\n",
    "\n",
    "Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically. The \"Choosing K\" section below describes how the number of groups can be determined.  \n",
    "\n",
    "Each centroid of a cluster is a collection of feature values which define the resulting groups. Examining the centroid feature weights can be used to qualitatively interpret what kind of group each cluster represents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business use:\n",
    "\n",
    "The K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group.\n",
    "\n",
    "This is a versatile algorithm that can be used for any type of grouping. Some examples of use cases are:\n",
    "\n",
    "1. Behavioral segmentation:\n",
    "    1. Segment by purchase history\n",
    "    2. Segment by activities on application, website, or platform\n",
    "    3. Define personas based on interests\n",
    "    4. Create profiles based on activity monitoring\n",
    "2. Inventory categorization:\n",
    "    1. Group inventory by sales activity\n",
    "    2. Group inventory by manufacturing metrics\n",
    "3. Sorting sensor measurements:\n",
    "    1. Detect activity types in motion sensors\n",
    "    2. Group images\n",
    "    3. Separate audio\n",
    "    4. Identify groups in health monitoring\n",
    "4. Detecting bots or anomalies:\n",
    "    1. Separate valid activity groups from bots\n",
    "    2. Group valid activity to clean up outlier detection\n",
    "\n",
    "In addition, monitoring if a tracked data point switches between groups over time can be used to detect meaningful changes in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm:\n",
    "\n",
    "The Κ-means clustering algorithm uses iterative refinement to produce a final result. The algorithm inputs are the number of clusters Κ and the data set. The data set is a collection of features for each data point. The algorithms starts with initial estimates for the Κ centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps:\n",
    "\n",
    "1. Data assigment step:\n",
    "\n",
    "Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, based on the squared Euclidean distance. More formally, if ${c_i}$ is the collection of centroids in set C, then each data point x is assigned to a cluster based on\n",
    "\n",
    "$$\n",
    "\\underset{c_{i} \\in C}{\\arg \\min } \\operatorname{dist}\\left(c_{i}, x\\right)^{2}\n",
    "$$\n",
    "\n",
    "where dist( · ) is the standard (L2) Euclidean distance. Let the set of data point assignments for each ith cluster centroid be Si.\n",
    "\n",
    "2. Centroid update step:\n",
    "\n",
    "In this step, the centroids are recomputed. This is done by taking the mean of all data points assigned to that centroid's cluster.\n",
    "\n",
    "$$\n",
    "c_{i}=\\frac{1}{\\left|S_{i}\\right|} \\sum_{x_{i} \\in S_{i}} x_{i}\n",
    "$$\n",
    "\n",
    "The algorithm iterates between steps one and two until a stopping criteria is met (i.e., no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached).\n",
    "\n",
    "This algorithm is guaranteed to converge to a result. The result may be a local optimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing K\n",
    "\n",
    "The algorithm described above finds the clusters and data set labels for a particular pre-chosen K. To find the number of clusters in the data, the user needs to run the K-means clustering algorithm for a range of K values and compare the results. In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following techniques.\n",
    "\n",
    "One of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the **\"elbow point,\"** where the rate of decrease sharply shifts, can be used to roughly determine K.\n",
    "\n",
    "A number of other techniques exist for validating K, including cross-validation, information criteria, the information theoretic jump method, the silhouette method, and the G-means algorithm. In addition, monitoring the distribution of data points across groups provides insight into how the algorithm is splitting the data for each K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
