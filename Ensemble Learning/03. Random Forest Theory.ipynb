{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Random forests** or **random decision forests** are an **ensemble learning method** for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. \n",
    "\n",
    "Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "The first algorithm for **random decision forests** was created by Tin Kam Ho (1995) using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n",
    "\n",
    "An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered **\"Random Forests\"** as a trademark (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's **\"bagging\"** idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n",
    "\n",
    "### Descriptions:\n",
    "\n",
    "Learning trees are very popular base models for ensemble methods. Strong learners composed of multiple trees can be called “forests”. Trees that compose a forest can be chosen to be either shallow (few depths) or deep (lot of depths, if not fully grown). Shallow trees have less variance but higher bias and then will be better choice for sequential methods that we will described thereafter. Deep trees, on the other side, have low bias but high variance and, so, are relevant choices for bagging method that is mainly focused at reducing variance.\n",
    "\n",
    "**The random forest approach is a bagging method where deep trees, fitted on bootstrap samples,** are combined to produce an output with lower variance. However, random forests also use another trick to make the multiple fitted trees a bit less correlated with each others: when growing each tree, instead of only sampling over the observations in the dataset to generate a bootstrap sample, we also sample over features and keep only a random subset of them to build the tree.\n",
    "(Use randomness along with features also)\n",
    "\n",
    "\n",
    "==> Random Forest is a versatile machine learning method capable of performing both regression and classification tasks. It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling over features has indeed the effect that all trees do not look at the exact same information to make their decisions and, so, it reduces the correlation between the different returned outputs. Another advantage of sampling over the features is that **it makes the decision making process more robust to missing data:** observations (from the training dataset or not) with missing data can still be regressed or classified based on the trees that take into account only features where data are not missing. Thus, *random forest algorithm combines the concepts of bagging and random feature subspace selection to create more robust models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest method is a bagging method with trees as weak learners. Each tree is fitted on a bootstrap sample considering only a subset of variables randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s look at the steps taken to implement Random forest:\n",
    "\n",
    "1. Suppose there are N observations and M features in training data set. First, a sample from training data set is taken randomly with replacement or forming of Bootstrap.\n",
    "2. A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively.\n",
    "3. The tree is grown to the largest.\n",
    "4. Above steps are repeated and prediction is given based on the aggregation of predictions from n number of trees.\n",
    "\n",
    "\n",
    "#### Advantages of using Random Forest technique:\n",
    "\n",
    "1. Handles higher dimensionality data very well.\n",
    "2. Handles missing values and maintains accuracy for missing data.\n",
    "\n",
    "#### Disadvantages of using Random Forest technique:\n",
    "\n",
    "1. Since final prediction is based on the mean predictions from subset trees, it won’t give precise values for the regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
