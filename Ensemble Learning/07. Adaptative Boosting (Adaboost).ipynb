{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptative Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Boosting\n",
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.\n",
    "\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is AdaBoost\n",
    "\n",
    "AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting.\n",
    "\n",
    "AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers\n",
    "\n",
    "AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple “weak classifiers” into a single “strong classifier”. Here are some (fun) facts about Adaboost!\n",
    "\n",
    "1. The weak learners in AdaBoost are decision trees with a single split, called decision stumps.\n",
    "2. AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.\n",
    "3. AdaBoost algorithms can be used for both classification and regression problem.\n",
    "\n",
    "## Learning An AdaBoost Model From Data\n",
    "\n",
    "AdaBoost is best used to boost the performance of decision trees on binary classification problems.\n",
    "\n",
    "AdaBoost was originally called AdaBoost.M1 by the authors of the technique Freund and Schapire. More recently it may be referred to as discrete AdaBoost because it is used for classification rather than regression.\n",
    "\n",
    "AdaBoost can be used to boost the performance of any machine learning algorithm. It is best used with weak learners. These are models that achieve accuracy just above random chance on a classification problem.\n",
    "\n",
    "The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps.\n",
    "\n",
    "Each instance in the training dataset is weighted. The initial weight is set to:\n",
    "\n",
    "weight(xi) = 1/n\n",
    "\n",
    "Where xi is the i’th training instance and n is the number of training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
