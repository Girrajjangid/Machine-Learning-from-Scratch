{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ensemble Methods?\n",
    "\n",
    "Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n",
    "\n",
    "Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. **The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner.** The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\n",
    "\n",
    "By analogy, ensemble techniques have been used also in **unsupervised learning** scenarios, for example in **consensus clustering** or in **anomaly detection.**\n",
    "\n",
    "\n",
    "An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data.\n",
    "\n",
    "Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine. Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees). Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.\n",
    "\n",
    "\n",
    "**\"the law of diminishing returns in ensemble construction.\" Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.**\n",
    "\n",
    "Ensemble modelling can exponentially boost the performance of your model and can sometimes be the deciding factor between first place and second in many competitions!\n",
    "\n",
    "Most of the errors from a model’s learning are from three main factors: **variance, noise, and bias.** By using ensemble methods, we’re able to increase the stability of the final model and reduce the errors mentioned previously. By combining many models, we’re able to (mostly) reduce the variance, even when they are individually not great, as we won’t suffer from random errors from a single source.\n",
    "\n",
    "\n",
    "### Small Description:\n",
    "\n",
    "Ensemble learning is a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "In ensemble learning theory, we call **weak learners (or base models) models** that can be used as building blocks for designing more complex models by combining several of them. Most of the time, these basics models perform not so well by themselves either because they have a high bias (low degree of freedom models, for example) or because they have too much variance to be robust (high degree of freedom models, for example). Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a **strong learner (or ensemble model)** that achieves better performances.\n",
    "\n",
    "### Combine weak learners\n",
    "\n",
    "In order to set up an ensemble learning method, we first need to select our base models to be aggregated. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have **homogeneous weak learners** that are trained in different ways. The ensemble model we obtain is then said to be “homogeneous”. However, there also exist some methods that use different type of base learning algorithms: some heterogeneous weak learners are then combined into an **“heterogeneous ensembles model”.**\n",
    "\n",
    "One important point is that our choice of weak learners should be **coherent with the way we aggregate these models.** If we choose base models with low bias but high variance, it should be with an aggregating method that tends to reduce variance(use bagging technique) whereas if we choose base models with low variance but high bias, it should be with an aggregating method that tends to reduce bias(use boosting technique).\n",
    "\n",
    "This brings us to the question of how to combine these models. We can mention three major kinds of meta-algorithms that aims at combining weak learners:\n",
    "\n",
    "**bagging,** that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "\n",
    "**boosting,** that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy\n",
    "\n",
    "**stacking,** that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions\n",
    "\n",
    "Very roughly, we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).\n",
    "\n",
    "**Bagging** to decrease the model’s variance.\n",
    "\n",
    "**Boosting** to decrease the model’s bias.\n",
    "\n",
    "**Stacking** to increasing the predictive force of the classifier.\n",
    "\n",
    "Weak learners can be combined to get a model with better performances. The way to combine base models should be adapted to their types. Low bias and high variance weak models should be combined in a way that makes the strong model more robust whereas low variance and high bias base models better be combined in a way that makes the ensemble model less biased.\n",
    "\n",
    "#### Ensemble learning is a two types :-\n",
    "\n",
    "1. Sequential Ensemble learning \n",
    "2. Parellel Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Ensemble methods\n",
    "\n",
    "1. Bayes optimal classifier\n",
    "2. Bootstrap aggregating (Bagging)\n",
    "3. Boosting\n",
    "4. Bayesian parameter averaging (BPA)\n",
    "5. Bayesian model combination (BMC)\n",
    "6. Bucket of model\n",
    "7. Stacking\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
