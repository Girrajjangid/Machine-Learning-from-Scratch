{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network \n",
    "\n",
    "A neural network usually takes an independent variable X (or a set of independent variables ) and a dependent variable y then it learns the mapping between X and y (we call this Training), Once training is done , we give a new independent variable to predict the dependent variable.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. \n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
    "\n",
    "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example input.\n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
    "    - Example: $x^{\\langle t \\rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\\langle t \\rangle}$ is the input at the $t^{th}$ timestep of example $i$.\n",
    "    \n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Forward propagation for the basic Recurrent Neural Network\n",
    "\n",
    "Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can implement an RNN: \n",
    "\n",
    "**Steps**:\n",
    "1. Implement the calculations needed for one time-step of the RNN.\n",
    "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. \n",
    "$T_x$ is just a length of your sequence\n",
    "\n",
    "Let's go!\n",
    "\n",
    "## 1.1 - RNN cell\n",
    "\n",
    "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided you a function: `softmax`.\n",
    "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in cache\n",
    "4. Return $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ and cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   <end>  banana  black  color  hair  has  is  mango  pink  yellow\n",
      "0      1       0      0      0     0    0   0      0     0       0\n",
      "1      0       1      0      0     0    0   0      0     0       0\n",
      "2      0       0      1      0     0    0   0      0     0       0\n",
      "3      0       0      0      1     0    0   0      0     0       0\n",
      "4      0       0      0      0     1    0   0      0     0       0\n",
      "5      0       0      0      0     0    1   0      0     0       0\n",
      "6      0       0      0      0     0    0   1      0     0       0\n",
      "7      0       0      0      0     0    0   0      1     0       0\n",
      "8      0       0      0      0     0    0   0      0     1       0\n",
      "9      0       0      0      0     0    0   0      0     0       1\n"
     ]
    }
   ],
   "source": [
    "# lets we have four sentences \n",
    "s1 = 'mango is yellow color'\n",
    "s2 = 'banana is pink color'\n",
    "s3 = 'hair has black color'\n",
    "\n",
    "# Now what is and how the matrices shapes define\n",
    "# create vocab\n",
    "l = [*s1.split(),*s2.split(),*s3.split()]\n",
    "l.append('<end>')\n",
    "vocab = sorted(set(l))\n",
    "one_hot_vector_vocab = np.array(pd.get_dummies(vocab))\n",
    "print(pd.get_dummies(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create two list which stores [character_to_index] values and [index_to_character] values\n",
    "\n",
    "char2idx = {value:index for index,value in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max length of time step\n",
    "\n",
    "seq_length = max(len(s1.split()),len(s2.split()),len(s3.split()))\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Input data for single batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single batch  (1*10*4) or (10*4) loop over 4 i.e loop (1*10) (1*10) (1*10) (1*10)\n",
    "\n",
    "##############################################################\n",
    "# input  #       shape      #   target   #       shape       #\n",
    "##############################################################\n",
    "# mango  (1*10) onehotvector     is      (1*10) onehotvector #\n",
    "# is     (1*10) onehotvector    yellow   (1*10) onehotvector #\n",
    "# yellow (1*10) onehotvector    color    (1*10) onehotvector #\n",
    "# color  (1*10) onehotvector    <end>    (1*10) onehotvector #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 9, 3]\n",
      "(10, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right now we pass single sentence or single batch so the input size will become \n",
    "#(1*10*5) batchsize * vocab_size * sequence_size or timestep \n",
    "print([char2idx[char] for char in s1.split()])\n",
    "print(one_hot_vector_vocab[[7,6,9,3]].T.shape)\n",
    "one_hot_vector_vocab[[7,6,9,3]].T  # mango is yellow color OH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mango', 'is', 'yellow', 'color'], dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input array for single batch\n",
    "np.array(s1.split()) # 4 words each represent in One Hot Encoding  dim = (10*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(10, 4)\n",
      "(10, 4)\n",
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare input data for single batch pass\n",
    "x1 = one_hot_vector_vocab[[char2idx[char] for char in s1.split()]].T #s1 = ' mango is yellow color '\n",
    "x2 = one_hot_vector_vocab[[char2idx[char] for char in s2.split()]].T #s2 = ' banana is pink color  '\n",
    "x3 = one_hot_vector_vocab[[char2idx[char] for char in s3.split()]].T #s3 = ' hair has black color  '\n",
    "print(x1.shape) # these are\n",
    "print(x2.shape) # three\n",
    "print(x3.shape) # different samples\n",
    "\n",
    "print(x1[:,0].shape) # 0th time step means we are passing mango as an input\n",
    "x1[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing target variable for single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'yellow' 'color' '<end>']\n",
      "['is' 'pink' 'color' '<end>']\n",
      "['has' 'black' 'color' '<end>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 9, 3, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = s1.split()[1:] # why I take from 1st index? bcz first input word predict 2nd so we expect 2 word from output\n",
    "y2 = s2.split()[1:]\n",
    "y3 = s3.split()[1:]\n",
    "y1.append('<end>') ; y2.append('<end>') ; y3.append('<end>') ;\n",
    "y1 = np.array(y1)\n",
    "y2 = np.array(y2)\n",
    "y3 = np.array(y3)\n",
    "print(y1) # these are\n",
    "print(y2) # three \n",
    "print(y3) # different samples\n",
    "[char2idx[i] for i in y1] # this is expected targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 3, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = np.array([char2idx[i] for i in y1]) # this is expected targets\n",
    "y2 = np.array([char2idx[i] for i in y2]) # this is expected targets\n",
    "y3 = np.array([char2idx[i] for i in y3]) # this is expected targets\n",
    "y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Single timestep feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    return np.exp(a)/np.exp(a).sum(axis=0)\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt         --  (batch_unit_size , vocab_size)\n",
    "    a_prev     --  (hidden_unit_size, vocab_size)\n",
    "\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- (batch_unit_size , hidden_unit_size)\n",
    "                        Waa -- (hidden_unit_size, hidden_unit_size)\n",
    "                        Wya -- (hidden_unit_size, output_unit_size)\n",
    "                        ba --  (hidden_unit_size, 1)\n",
    "                        by --  (output_unit_size, 1)\n",
    "    Returns:\n",
    "    a_next  -- (hidden_unit, vocab_size)\n",
    "    yt_pred -- (output_unit, vocab_size)\n",
    "    cache   -- (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Way = parameters[\"Way\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa.T, a_prev) + np.dot(Wax.T, xt) + ba) # current activation bias\n",
    "    \n",
    "    y_cap = softmax(np.dot(Way.T, a_next) + by)  # output of current time step\n",
    "    \n",
    "    cache = (a_prev, xt, a_next, parameters) # need during back propagation \n",
    "    \n",
    "    return a_next, y_cap, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:,0] # mango\n",
    "x1[:,1] # is \n",
    "x1[:,2] # yellow\n",
    "x1[:,3] # color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           Wax.T  *  Xt               Way.T  * a_current\n",
    "# (1*10)    (2*1)  * (1*10)             (1*2) *  (2*10)\n",
    "# mango ---------------------> (2*10) --------------------->  is = (1*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next =  (2, 10)\n",
      "a_next.shape =  (2, 10)\n",
      "y_cap.shape =  (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# we create 1 hidden layer with 2 neurons\n",
    "np.random.seed(1)\n",
    "xt     = x1[:,0].reshape(1,-1)  # (1*10)  ==> batch_size    * vocab_size \n",
    "Wax    = np.random.randn(1,2)   # (1*2)   ==> batch_size    * hidden_neuron\n",
    "\n",
    "a_prev = np.random.randn(2,10)  # (2*10)  ==> hidden_neuron * vocab_size\n",
    "Waa    = np.random.randn(2,2)   # (2*2)   ==> hidden_neuron * hidden_neuron\n",
    "ba     = np.random.randn(2,1)   # (2*1)   ==> hidden nueron * 1\n",
    "\n",
    "Way    = np.random.randn(2,1)   # (2*1)   ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by     = np.random.randn(1,1)   # (1*1)   ==> output_neuron * 1\n",
    "\n",
    "# in sentiment anlysis output_neron will be no. of labels \n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, y_cap, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next = \", a_next.shape)\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"y_cap.shape = \", y_cap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dont expect good output because we dont train this RNN. I just explain whole code with short sample \n",
    "y_cap  # index with max value would be next possible word but here all are possible. lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Multi timestep feedforward \n",
    "#### Don't think that it's a multi neuron like NN. It just a single neuron where activations are feed in loop \n",
    "\n",
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 4 time steps, then you will copy the RNN cell 4 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt         --  (batch_unit_size , vocab_size , timestep)\n",
    "    a_prev     --  (hidden_unit_size, vocab_size)\n",
    "\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- (batch_unit_size , hidden_unit_size)\n",
    "                        Waa -- (hidden_unit_size, hidden_unit_size)\n",
    "                        Way -- (hidden_unit_size, output_unit_size)\n",
    "                        ba --  (hidden_unit_size, 1)\n",
    "                        by --  (output_unit_size, 1)\n",
    "    Returns:\n",
    "    a_next  -- (hidden_unit, vocab_size)\n",
    "    yt_pred -- (output_unit, vocab_size)\n",
    "    cache   -- (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []  # store outputs which we get using single timestep  [need during backpropagation]\n",
    "    \n",
    "    hidden_neuron , vocab_size         =  a_prev.shape\n",
    "    batch_size , vocab_size , timestep =  xt.shape\n",
    "    hidden_neuron, output_neuron       = parameters[\"Way\"].shape\n",
    "    \n",
    "    activations = np.zeros([hidden_neuron,vocab_size,timestep])  # store activations during each timesteps\n",
    "    y_pred      = np.zeros([output_neuron,vocab_size,timestep])  # store output at each timesteps\n",
    "    \n",
    "    a_next = a_prev  # why? you will get soon\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(timestep):\n",
    "        \n",
    "        a_next, y_cap, cache = rnn_cell_forward(xt[:,:,t], a_next, parameters)\n",
    "        activations[:,:,t]   = a_next\n",
    "        y_pred[:,:,t]        = y_cap\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, xt)\n",
    "    \n",
    "    return activations, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations.shape =  (2, 10, 4)\n",
      "y_caps.shape      =  (1, 10, 4)\n",
      "len(caches)       =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt     = x1.reshape(1,10,4)     # (1*10*4) ==> batch_size    * vocab_size * timestep \n",
    "Wax    = np.random.randn(1,2)   # (1*2)    ==> batch_size    * hidden_neuron\n",
    "\n",
    "a_prev = np.random.randn(2,10)  # (2*10)   ==> hidden_neuron * vocab_size\n",
    "Waa    = np.random.randn(2,2)   # (2*2)    ==> hidden_neuron * hidden_neuron\n",
    "ba     = np.random.randn(2,1)   # (2*1)    ==> hidden nueron * 1\n",
    "\n",
    "Way    = np.random.randn(2,1)   # (2*1)    ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by     = np.random.randn(1,1)   # (1*1)    ==> output_neuron * 1\n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "activations, y_caps, caches = rnn_forward(xt, a_prev, parameters)\n",
    "print(\"activations.shape = \", activations.shape)\n",
    "print(\"y_caps.shape      = \", y_caps.shape)\n",
    "print(\"len(caches)       = \", len(caches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 1, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches    # len = 2 it is tuple which contains (cache , xt)\n",
    "caches[0] # len = 4 (4 time step data)\n",
    "caches[1] # len = (1*10*4) input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.52817175, -1.07296862,  0.86540763, -2.3015387 ,  1.74481176,\n",
       "         -0.7612069 ,  0.3190391 , -0.24937038,  1.46210794, -2.06014071],\n",
       "        [-0.3224172 , -0.38405435,  1.13376944, -1.09989127, -0.17242821,\n",
       "         -0.87785842,  0.04221375,  0.58281521, -1.10061918,  1.14472371]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]], dtype=uint8),\n",
       " array([[-0.71116469, -0.89293958,  0.93269476, -0.99660724,  0.86040002,\n",
       "         -0.92167025,  0.20004648,  0.94697741,  0.20105657, -0.73935847],\n",
       "        [-0.7533805 , -0.83738047, -0.85544148, -0.87173307,  0.05881461,\n",
       "         -0.61570329, -0.66644319, -0.96873478,  0.5016103 , -0.99191882]]),\n",
       " {'Waa': array([[ 0.90159072,  0.50249434],\n",
       "         [ 0.90085595, -0.68372786]]),\n",
       "  'Wax': array([[ 1.62434536, -0.61175641]]),\n",
       "  'Way': array([[-0.26788808],\n",
       "         [ 0.53035547]]),\n",
       "  'ba': array([[-0.12289023],\n",
       "         [-0.93576943]]),\n",
       "  'by': array([[-0.69166075]])})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0][0] # first time step type is tuple (a_prev , xt , a_next , parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.71116469, -0.89293958,  0.93269476, -0.99660724,  0.86040002,\n",
       "         -0.92167025,  0.20004648,  0.94697741,  0.20105657, -0.73935847],\n",
       "        [-0.7533805 , -0.83738047, -0.85544148, -0.87173307,  0.05881461,\n",
       "         -0.61570329, -0.66644319, -0.96873478,  0.5016103 , -0.99191882]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=uint8),\n",
       " array([[-0.89425136, -0.93316138, -0.05256234, -0.94749833,  0.6080502 ,\n",
       "         -0.90667597,  0.79373466, -0.14085196,  0.47014708, -0.93325815],\n",
       "        [-0.65156786, -0.67065145,  0.11725177, -0.68609008, -0.49573571,\n",
       "         -0.75216843, -0.75793221,  0.19971175, -0.82672644, -0.55742553]]),\n",
       " {'Waa': array([[ 0.90159072,  0.50249434],\n",
       "         [ 0.90085595, -0.68372786]]),\n",
       "  'Wax': array([[ 1.62434536, -0.61175641]]),\n",
       "  'Way': array([[-0.26788808],\n",
       "         [ 0.53035547]]),\n",
       "  'ba': array([[-0.12289023],\n",
       "         [-0.93576943]]),\n",
       "  'by': array([[-0.69166075]])})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0][1] # second time step type is tuple (a_prev , xt , a_next , parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.89425136, -0.93316138, -0.05256234, -0.94749833,  0.6080502 ,\n",
       "         -0.90667597,  0.79373466, -0.14085196,  0.47014708, -0.93325815],\n",
       "        [-0.65156786, -0.67065145,  0.11725177, -0.68609008, -0.49573571,\n",
       "         -0.75216843, -0.75793221,  0.19971175, -0.82672644, -0.55742553]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=uint8),\n",
       " array([[-0.90801696, -0.91676773, -0.06456306, -0.92094498, -0.02126107,\n",
       "         -0.92432426, -0.08981151, -0.06985556, -0.41676567,  0.15657936],\n",
       "        [-0.73505248, -0.73802812, -0.77881437, -0.73649832, -0.28331189,\n",
       "         -0.70495873, -0.01870071, -0.81545373, -0.13346623, -0.92682048]]),\n",
       " {'Waa': array([[ 0.90159072,  0.50249434],\n",
       "         [ 0.90085595, -0.68372786]]),\n",
       "  'Wax': array([[ 1.62434536, -0.61175641]]),\n",
       "  'Way': array([[-0.26788808],\n",
       "         [ 0.53035547]]),\n",
       "  'ba': array([[-0.12289023],\n",
       "         [-0.93576943]]),\n",
       "  'by': array([[-0.69166075]])})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0][2] # third time step type is tuple (a_prev , xt , a_next , parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.90801696, -0.91676773, -0.06456306, -0.92094498, -0.02126107,\n",
       "         -0.92432426, -0.08981151, -0.06985556, -0.41676567,  0.15657936],\n",
       "        [-0.73505248, -0.73802812, -0.77881437, -0.73649832, -0.28331189,\n",
       "         -0.70495873, -0.01870071, -0.81545373, -0.13346623, -0.92682048]]),\n",
       " array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=uint8),\n",
       " array([[-0.92222754, -0.92379244, -0.70776899,  0.00766064, -0.37762112,\n",
       "         -0.92035127, -0.21719476, -0.72612332, -0.55034504, -0.67324321],\n",
       "        [-0.71113035, -0.71229625, -0.41008627, -0.90635722, -0.63678359,\n",
       "         -0.72506242, -0.74787389, -0.39129058, -0.78333237, -0.21975267]]),\n",
       " {'Waa': array([[ 0.90159072,  0.50249434],\n",
       "         [ 0.90085595, -0.68372786]]),\n",
       "  'Wax': array([[ 1.62434536, -0.61175641]]),\n",
       "  'Way': array([[-0.26788808],\n",
       "         [ 0.53035547]]),\n",
       "  'ba': array([[-0.12289023],\n",
       "         [-0.93576943]]),\n",
       "  'by': array([[-0.69166075]])})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0][3] # forth time step type is tuple (a_prev , xt , a_next , parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "<img src=\"images/costfunction.jpeg\" style=\"width:500;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(y_caps[:,:,0]))\n",
    "print(np.argmax(y_caps[:,:,1]))\n",
    "print(np.argmax(y_caps[:,:,2]))\n",
    "print(np.argmax(y_caps[:,:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([np.argmax(y_caps[:,:,0]),np.argmax(y_caps[:,:,1]),np.argmax(y_caps[:,:,2]),np.argmax(y_caps[:,:,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y1\n",
    "np.argmax(y_caps[:,:,0])\n",
    "np.argmax(y_caps[:,:,1])\n",
    "np.argmax(y_caps[:,:,2])\n",
    "np.argmax(y_caps[:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfunction(y_pred, y_actual):\n",
    "    return (1/4)*(np.dot(y_actual,np.log(y_pred)))\n",
    "#costfunction(y_pred,y1)\n",
    "#np.log(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Basic RNN  backward pass\n",
    "\n",
    "We will start by computing the backward pass for the basic RNN-cell.\n",
    "\n",
    "<img src=\"images/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 6**: RNN-cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculus. The chain-rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache   -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx      -- Gradients of input data, of shape              (batch_size ,vocab_size, timestep)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape   (hidden_unit,vocab_size)\n",
    "                        dWax    -- Gradients of input-to-hidden weights, of shape (batch_size ,hidden_unit)\n",
    "                        dWaa    -- Gradients of hidden-to-hidden weights, of shape(hidden_unit,hidden_unit)\n",
    "                        dba     -- Gradients of bias vector, of shape             (hidden_unit, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from cache\n",
    "    (a_prev, xt, a_next, parameters) = cache # (2*10),(1*10),(2*10),\n",
    "                                            # (Waa,Wax,Way,ba,by) = (2*2),(1*2),(2*1),(2*1),(1*1)\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Way = parameters[\"Way\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    # compute the gradient of tanh with respect to a_next \n",
    "    da_dtanh = (1 - np.square(a_next))  # a_next = np.tanh(x) we calculate this in forward pass  (2*10)\n",
    "    \n",
    "                    #(1*2)  (2*10)\n",
    "    da_dxt    = np.dot(Wax , da_dtanh)  # (1*10)\n",
    "                    #(2*2)  (2*10)   \n",
    "    da_da_prev = np.dot(Waa , da_dtanh)  # (2*10)\n",
    "    \n",
    "                    #(1*10) (2*10).T\n",
    "    da_dWax   = np.dot(xt, da_dtanh.T)     # (1*2)\n",
    "                    #(2*10)  (2*10).T\n",
    "    da_dWaa   = np.dot(a_prev, da_dtanh.T) # (2*2)\n",
    "\n",
    "                    #(2*10)\n",
    "    da_dba = np.sum(da_dtanh,axis = 1).reshape(-1,1)  # (2*1)\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"da_dxt\": da_dxt, \"da_da_prev\": da_da_prev, \"da_dWax\": da_dWax, \"da_dWaa\": da_dWaa, \"da_dba\": da_dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"].shape = (1, 10)\n",
      "gradients[\"da_prev\"].shape = (2, 10)\n",
      "gradients[\"dWax\"].shape = (1, 2)\n",
      "gradients[\"dWaa\"].shape = (2, 2)\n",
      "gradients[\"dba\"].shape = (2, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt     = x1.reshape(1,10,4)[:,:,0] # (1*10*4) ==> batch_size    * vocab_size * timestep \n",
    "Wax    = np.random.randn(1,2)   # (1*2)    ==> batch_size    * hidden_neuron\n",
    "\n",
    "a_prev = np.random.randn(2,10)  # (2*10)   ==> hidden_neuron * vocab_size\n",
    "Waa    = np.random.randn(2,2)   # (2*2)    ==> hidden_neuron * hidden_neuron\n",
    "ba     = np.random.randn(2,1)   # (2*1)    ==> hidden nueron * 1\n",
    "\n",
    "Way    = np.random.randn(2,1)   # (2*1)    ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by     = np.random.randn(1,1)   # (1*1)    ==> output_neuron * 1\n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "activations, y_caps, caches = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(2,10)\n",
    "gradients = rnn_cell_backward(da_next , caches) # da_dxt, da_da_prev, da_dWax, da_dWaa, da_dba\n",
    "\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"da_dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"da_dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"da_dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"da_dba\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da     -- Upstream gradients of all hidden states, of shape (batch_size, vocab_size, timestep)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                    dx  -- Gradient w.r.t. the input data, numpy-array of shape (batch_size, vocab_size, timestep)\n",
    "                    da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (hidden_unit, vocab_size)\n",
    "                    dWax-- Gradient w.r.t the input's weight matrix, numpy-array of shape (batch_size, hidden_unit)\n",
    "                    dWaa-- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (hidden_unit,hidden_unit)\n",
    "                    dba -- Gradient w.r.t the bias, of shape (hidden_unit, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from the first cache (t=1) of caches\n",
    "    (caches, x) = caches # all time step data ===> where x contains a_next of all timesteps \n",
    "                                             # caches contains a_prev,xt,a_next,parameters for all timesteps\n",
    "    (a0, x1, a1, parameters) = caches[0] # first time step data\n",
    "    \n",
    "    \n",
    "    batch_size, vocab_size =  x1.shape\n",
    "    timestep = len(caches) \n",
    "    hidden_unit, vocab_size = a0.shape\n",
    "    \n",
    "    dx   = np.zeros([batch_size,vocab_size,timestep])\n",
    "    dWax = 0\n",
    "    dWaa = 0\n",
    "    dba  = 0\n",
    "    da0  = 0\n",
    "    da_prevt = 0\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(timestep)):\n",
    "        # Compute gradients at time step t. \n",
    "        \n",
    "        gradients = rnn_cell_backward(da[:,:,t] , caches[t])\n",
    "        da_dxtt, da_da_prevt = gradients[\"da_dxt\"], gradients[\"da_da_prev\"],\n",
    "        da_dWaxt, da_dWaat, da_dbat =  gradients[\"da_dWax\"], gradients[\"da_dWaa\"], gradients[\"da_dba\"]\n",
    "        \n",
    "        dx[:, :, t] = da_dxtt\n",
    "        dWax += da_dWaxt\n",
    "        dWaa += da_dWaat\n",
    "        dba += da_dbat\n",
    "        da_prevt += da_da_prevt\n",
    "    \n",
    "    da0 = da_prevt\n",
    "    \n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations.shape =  (2, 10, 4)\n",
      "y_caps.shape      =  (1, 10, 4)\n",
      "len(caches)       =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt  = x1.reshape(1,10,4)     # (1*10*4) ==> batch_size    * vocab_size * timestep \n",
    "Wax = np.random.randn(1,2)   # (1*2)    ==> batch_size    * hidden_neuron\n",
    "\n",
    "a0  = np.random.randn(2,10)  # (2*10)   ==> hidden_neuron * vocab_size\n",
    "Waa = np.random.randn(2,2)   # (2*2)    ==> hidden_neuron * hidden_neuron\n",
    "ba  = np.random.randn(2,1)   # (2*1)    ==> hidden nueron * 1\n",
    "\n",
    "Way = np.random.randn(2,1)   # (2*1)    ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by  = np.random.randn(1,1)   # (1*1)    ==> output_neuron * 1\n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "activations, y_caps, caches = rnn_forward(xt, a0, parameters)\n",
    "print(\"activations.shape = \", activations.shape)\n",
    "print(\"y_caps.shape      = \", y_caps.shape)\n",
    "print(\"len(caches)       = \", len(caches))\n",
    "\n",
    "da = np.random.randn(2, 10, 4)\n",
    "gradients = rnn_backward(da, caches) # caches = da = [d(a_t+1)*4   (at, at-1, xt, parameter)]\n",
    "\n",
    "#print(\"gradients[\\\"dx\\\"].shape =\",   gradients[\"dx\"].shape)\n",
    "#print(\"gradients[\\\"da0\\\"].shape =\",  gradients[\"da0\"].shape)\n",
    "#print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "#print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "#print(\"gradients[\\\"dba\\\"].shape =\",  gradients[\"dba\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dWaa': array([[  1.11129101,   0.11771204],\n",
       "        [ -8.20854707, -12.76779123]]),\n",
       " 'dWax': array([[2.44864329, 0.80661209]]),\n",
       " 'da0': array([[ 1.90505448,  1.4781936 ,  3.61022234,  1.84297594,  4.117271  ,\n",
       "          1.58663874,  4.16936769,  3.40748605,  4.16377087,  2.80102035],\n",
       "        [-0.42290677, -0.65387985,  0.6667619 ,  0.17487511,  0.24220396,\n",
       "         -0.82478276,  1.29425499,  0.7916581 ,  1.27471698,  0.66654395]]),\n",
       " 'dba': array([[20.10714295],\n",
       "        [21.79842979]]),\n",
       " 'dx': array([[[ 0.53828984, -0.02666099,  0.00385796, -0.05955356],\n",
       "         [ 0.14639895, -0.12672326, -0.01939849, -0.06323078],\n",
       "         [ 0.04721041,  1.01651162,  1.37688001,  0.30177389],\n",
       "         [-0.13586802, -0.15770639, -0.03324789,  1.51504137],\n",
       "         [-0.18777859,  0.56236931,  1.0609577 ,  1.02902418],\n",
       "         [-0.13534247,  0.02338314, -0.07118904, -0.04169721],\n",
       "         [ 1.21929442,  0.34065781,  0.99970075,  1.27812755],\n",
       "         [ 0.13003192,  1.00476289,  1.41145894,  0.24980961],\n",
       "         [ 1.10085263,  1.07166764,  0.74134787,  0.89598738],\n",
       "         [ 0.72654796, -0.21208147,  1.49826115,  0.30588646]]])}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - LSTM backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 One Step backward\n",
    "\n",
    "The LSTM backward pass is slighltly more complicated than the forward one. We have provided you with all the equations for the LSTM backward pass below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.) \n",
    "\n",
    "### 3.2.2 gate derivatives\n",
    "\n",
    "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
    "\n",
    "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
    "\n",
    "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
    "\n",
    "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
    "\n",
    "### 3.2.3 parameter derivatives \n",
    "\n",
    "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
    "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
    "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
    "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
    "\n",
    "To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\\Gamma_f^{\\langle t \\rangle}, d\\Gamma_u^{\\langle t \\rangle}, d\\tilde c^{\\langle t \\rangle}, d\\Gamma_o^{\\langle t \\rangle}$ respectively. Note that you should have the `keep_dims = True` option.\n",
    "\n",
    "Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input.\n",
    "\n",
    "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
    "Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...)\n",
    "\n",
    "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
    "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$\n",
    "where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...)\n",
    "\n",
    "**Exercise:** Implement `lstm_cell_backward` by implementing equations $7-17$ below. Good luck! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]:\n",
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) / temperature # shift each row of y_linear by its max\n",
    "    exp = nd.exp(lin)\n",
    "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
    "    return exp / partition\n",
    "\n",
    " def cross_entropy(yhat, y):\n",
    "     return - nd.sum(y * nd.log(yhat))\n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
    "\n",
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve information from \"cache\"\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
    "    n_x, m = None\n",
    "    n_a, m = None\n",
    "    \n",
    "    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)\n",
    "    dot = None\n",
    "    dcct = None\n",
    "    dit = None\n",
    "    dft = None\n",
    "    \n",
    "    # Code equations (7) to (10) (≈4 lines)\n",
    "    dit = None\n",
    "    dft = None\n",
    "    dot = None\n",
    "    dcct = None\n",
    "\n",
    "    # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)\n",
    "    dWf = None\n",
    "    dWi = None\n",
    "    dWc = None\n",
    "    dWo = None\n",
    "    dbf = None\n",
    "    dbi = None\n",
    "    dbc = None\n",
    "    dbo = None\n",
    "\n",
    "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)\n",
    "    da_prev = None\n",
    "    dc_prev = None\n",
    "    dxt = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "dc_next = np.random.randn(5,10)\n",
    "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients[\"dc_prev\"][2][3])\n",
    "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients[\"dc_prev\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           3.23055911511\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0639621419711\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "         <tr>\n",
    "        <td>\n",
    "            **gradients[\"dc_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.797522038797\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dc_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.147954838164\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           1.05749805523\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           2.30456216369\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.331311595289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.18864637]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.40142491]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.25587763]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.13893342]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Backward pass through the LSTM RNN\n",
    "\n",
    "This part is very similar to the `rnn_backward` function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. \n",
    "\n",
    "**Instructions**: Implement the `lstm_backward` function. Create a for loop starting from $T_x$ and going backward. For each step call `lstm_cell_backward` and update the your old gradients by adding the new gradients to them. Note that `dxt` is not updated but is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
    "\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = None\n",
    "    n_x, m = None\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈12 lines)\n",
    "    dx = None\n",
    "    da0 = None\n",
    "    da_prevt = None\n",
    "    dc_prevt = None\n",
    "    dWf = None\n",
    "    dWi = None\n",
    "    dWc = None\n",
    "    dWo = None\n",
    "    dbf = None\n",
    "    dbi = None\n",
    "    dbc = None\n",
    "    dbo = None\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(None)):\n",
    "        # Compute all gradients using lstm_cell_backward\n",
    "        gradients = None\n",
    "        # Store or add the gradient to the parameters' previous step's gradient\n",
    "        dx[:,:,t] = None\n",
    "        dWf = None\n",
    "        dWi = None\n",
    "        dWc = None\n",
    "        dWo = None\n",
    "        dbf = None\n",
    "        dbi = None\n",
    "        dbc = None\n",
    "        dbo = None\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.00173313  0.08287442 -0.30545663 -0.43281115]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.095911501954\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0698198561274\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.102371820249\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0624983794927\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.0484389131444\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.0565788]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.06997391]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.27441821]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.16532821]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations !\n",
    "\n",
    "Congratulations on completing this assignment. You now understand how recurrent neural networks work! \n",
    "\n",
    "Let's go on to the next exercise, where you'll use an RNN to build a character-level language model.\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
