{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   <end>  banana  black  color  hair  has  is  mango  pink  yellow\n",
      "0      1       0      0      0     0    0   0      0     0       0\n",
      "1      0       1      0      0     0    0   0      0     0       0\n",
      "2      0       0      1      0     0    0   0      0     0       0\n",
      "3      0       0      0      1     0    0   0      0     0       0\n",
      "4      0       0      0      0     1    0   0      0     0       0\n",
      "5      0       0      0      0     0    1   0      0     0       0\n",
      "6      0       0      0      0     0    0   1      0     0       0\n",
      "7      0       0      0      0     0    0   0      1     0       0\n",
      "8      0       0      0      0     0    0   0      0     1       0\n",
      "9      0       0      0      0     0    0   0      0     0       1\n"
     ]
    }
   ],
   "source": [
    "# lets we have four sentences \n",
    "s1 = 'mango is yellow color'\n",
    "s2 = 'banana is pink color'\n",
    "s3 = 'hair has black color'\n",
    "\n",
    "# Now what is and how the matrices shapes define\n",
    "# create vocab\n",
    "l = [*s1.split(),*s2.split(),*s3.split()]\n",
    "l.append('<end>')\n",
    "vocab = sorted(set(l))\n",
    "one_hot_vector_vocab = np.array(pd.get_dummies(vocab))\n",
    "print(pd.get_dummies(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create two list which stores [character_to_index] values and [index_to_character] values\n",
    "char2idx = {value:index for index,value in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max length of time step\n",
    "seq_length = max(len(s1.split()),len(s2.split()),len(s3.split()))\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Input for multi batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For three batch batch  (3*10*4) loop 4 times i.e (3*10) (3*10) (3*10) (3*10)\n",
    "\n",
    "############################################################################################\n",
    "#          input        #       shape       #        target          #       shape         #\n",
    "############################################################################################\n",
    "# [mango ,banana,hair ]  (3*10) onehotvector    [  is  , is  , has ]   (1*10) onehotvector #\n",
    "# [ is   ,  is  ,has  ]  (3*10) onehotvector    [yellow,pink ,black]   (1*10) onehotvector #\n",
    "# [yellow,pink  ,black]  (3*10) onehotvector    [color ,color,color]   (1*10) onehotvector #\n",
    "# [color ,color ,color]  (3*10) onehotvector    [<end> ,<end>,<end>]   (1*10) onehotvector #\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input array for multi batch size\n",
    "x1 = np.array([s1.split(),s2.split(),s3.split()]).T[0] \n",
    "x2 = np.array([s1.split(),s2.split(),s3.split()]).T[1]\n",
    "x3 = np.array([s1.split(),s2.split(),s3.split()]).T[2]\n",
    "x4 = np.array([s1.split(),s2.split(),s3.split()]).T[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 4]\n",
      "[6, 6, 5]\n",
      "[9, 8, 2]\n",
      "[3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print([char2idx[char] for char in x1])  # first time step\n",
    "print([char2idx[char] for char in x2])  # second time step\n",
    "print([char2idx[char] for char in x3])  # third time step\n",
    "print([char2idx[char] for char in x4])  # forth time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing target variable for multi batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing target varialbes\n",
    "# target predict the next upcoming words so we only have to expect 3 prediction words and 4 word is end of sentence\n",
    "y1 = np.array([s1.split(),s2.split(),s3.split()]).T[1]\n",
    "y2 = np.array([s1.split(),s2.split(),s3.split()]).T[2]\n",
    "y3 = np.array([s1.split(),s2.split(),s3.split()]).T[3]\n",
    "y4 = np.array(['<end>','<end>','<end>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 6, 5]\n",
      "[9, 8, 2]\n",
      "[3, 3, 3]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print([char2idx[i] for i in y1]) # this is expected targets for first time step\n",
    "print([char2idx[i] for i in y2]) # this is expected targets for second time step\n",
    "print([char2idx[i] for i in y3]) # this is expected targets for third time step\n",
    "print([char2idx[i] for i in y4]) # this is expected targets for forth time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mango' 'banana' 'hair'] ['is' 'is' 'has'] ['yellow' 'pink' 'black'] ['color' 'color' 'color']\n",
      "['is' 'is' 'has'] ['yellow' 'pink' 'black'] ['color' 'color' 'color'] ['<end>' '<end>' '<end>']\n"
     ]
    }
   ],
   "source": [
    "print(x1,x2,x3,x4)\n",
    "print(y1,y2,y3,y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# input_neuron(3)     ###  1_H_l__2_neuron(2)    #### out_lay_3_neuron(3)        #\n",
    "##################################################################################\n",
    "#                      Wax.T  *  Xt               Way.T  * a_current             #\n",
    "# (3*10)               (2*3)  * (3*10)             (3*2) *  (2*10)               #\n",
    "# mango       ----------------------->        --------------------->  is         #\n",
    "# banana      -----------------------> (2*10) --------------------->  is  (3*10) #\n",
    "# hair        ----------------------->        --------------------->  has        #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    return np.exp(a)/np.exp(a).sum(axis=1).reshape(-1,1)\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt         --  (batch_unit_size , vocab_size)\n",
    "    a_prev     --  (hidden_unit_size, vocab_size)\n",
    "\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- (batch_unit_size , hidden_unit_size)\n",
    "                        Waa -- (hidden_unit_size, hidden_unit_size)\n",
    "                        Wya -- (hidden_unit_size, output_unit_size)\n",
    "                        ba --  (hidden_unit_size, 1)\n",
    "                        by --  (output_unit_size, 1)\n",
    "    Returns:\n",
    "    a_next  -- (hidden_unit, vocab_size)\n",
    "    yt_pred -- (output_unit, vocab_size)\n",
    "    cache   -- (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Way = parameters[\"Way\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa.T, a_prev) + np.dot(Wax.T, xt) + ba) # current activation bias\n",
    "    y_cap  = softmax(np.dot(Way.T, a_next) + by)  # output of current time step\n",
    "    return (a_prev, a_next, y_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt         --  (batch_unit_size , vocab_size , timestep)\n",
    "    a_prev     --  (hidden_unit_size, vocab_size)\n",
    "\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- (batch_unit_size , hidden_unit_size)\n",
    "                        Waa -- (hidden_unit_size, hidden_unit_size)\n",
    "                        Way -- (hidden_unit_size, output_unit_size)\n",
    "                        ba --  (hidden_unit_size, 1)\n",
    "                        by --  (output_unit_size, 1)\n",
    "    Returns:\n",
    "    a_next  -- (hidden_unit, vocab_size)\n",
    "    yt_pred -- (output_unit, vocab_size)\n",
    "    cache   -- (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []  # store outputs which we get using single timestep  [need during backpropagation]\n",
    "    \n",
    "    hidden_neuron , vocab_size         =  a_prev.shape\n",
    "    batch_size , vocab_size , timestep =  xt.shape\n",
    "    hidden_neuron, output_neuron       =  parameters[\"Way\"].shape\n",
    "    \n",
    "    activations = np.zeros([hidden_neuron,vocab_size,timestep])  # store activations during each timesteps\n",
    "    y_caps      = np.zeros([output_neuron,vocab_size,timestep])  # store output at each timesteps\n",
    "\n",
    "    a_next_t = a_prev  # why? you will get soon\n",
    "    a0 = 0\n",
    "    # loop over all time-steps\n",
    "    for t in range(timestep):\n",
    "        \n",
    "        a_prev_t, a_next_t, y_cap_t = rnn_cell_forward(xt[:,:,t], a_next_t, parameters)\n",
    "        if t == 0:\n",
    "            a0 = a_prev_t # store only first prev activation\n",
    "        activations[:,:,t]   = a_next_t\n",
    "        y_caps[:,:,t]        = y_cap_t\n",
    "    \n",
    "    return a0, activations, y_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(timestep, y_caps, y, activation, parameters,xt,a0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache   -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx      -- Gradients of input data, of shape              (batch_size ,vocab_size, timestep)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape   (hidden_unit,vocab_size)\n",
    "                        dWax    -- Gradients of input-to-hidden weights, of shape (batch_size ,hidden_unit)\n",
    "                        dWaa    -- Gradients of hidden-to-hidden weights, of shape(hidden_unit,hidden_unit)\n",
    "                        dba     -- Gradients of bias vector, of shape             (hidden_unit, 1)\n",
    "    \"\"\"\n",
    "    Wax = parameters[\"Wax\"] #(3*2)\n",
    "    Waa = parameters[\"Waa\"] #(2*2)\n",
    "    Way = parameters[\"Way\"] #(2*3)\n",
    "    ba  = parameters[\"ba\"]  #(2*1)\n",
    "    by  = parameters[\"by\"]  #(3*1)\n",
    "    \n",
    "    y_cap_ = y_caps[:,:,timestep]      # current timestep y_pred\n",
    "    y_     = y[:,:,timestep]           # current timestep y_actual\n",
    "    a_     = activation[:,:,timestep]  # current timestep a_next\n",
    "    xt_    = xt[:,:,timestep]          # current timestep input\n",
    "    a0_    = a0                        # a_prev for first timestep\n",
    "    \n",
    "    # compute derivative Way w.r.t to Loss\n",
    "    #                (2*10) (3*10)-(3*10)\n",
    "    dL_dWay = np.dot( a_,  (y_cap_ - y_).T)                # (2*3) \n",
    "    \n",
    "    # dL_dWaa = dL_da * da__da_prev * da_da_prev__dWaa \n",
    "    dL_dWaa = 0\n",
    "    for i in range(timestep+1):   \n",
    "        #                    (2*3)  (3*10)-(3*10) \n",
    "        dL_da        = np.dot(Way, (y_cap_ - y_))          # (2*10)\n",
    "        \n",
    "        da__da_prev = 1\n",
    "        for j in reversed(range(i+1,timestep+1)): \n",
    "            #                               (2*10)\n",
    "            da_dtanh   = (1-np.square(activation[:,:,j]))   # (2*10)\n",
    "            #                               (2*2)  (2*10)   \n",
    "            da__da_prev= da__da_prev * np.dot(Waa , da_dtanh)# (2*10)\n",
    "            \n",
    "        if i == 0: \n",
    "            #           (2*10)                    (2*10)  \n",
    "            da_dWaa   =  a0_   *  (1 - np.square(activation[:,:,i])) # (2*10)        \n",
    "        else:\n",
    "            #                          (2*10)                               (2*10)\n",
    "            da_dWaa   =  (1 - np.square(activation[:,:,i-1]))   *  (1 - np.square(activation[:,:,i])) # (2*10)\n",
    "            \n",
    "            #            (2*10)     (2*10)       (10*2)\n",
    "        dL_dWaa += np.dot(dL_da * da__da_prev , da_dWaa.T)  # (2*2)\n",
    "    \n",
    "    # dL_dWax = dL_da * da__da_prev * da_da_prev__dWax \n",
    "    dL_dWax = 0\n",
    "    da__da_prev = 0\n",
    "    for i in range(timestep+1):  \n",
    "        dL_da       = np.dot(Way, (y_cap_ - y_))   # (2*10)\n",
    "        da__da_prev = np.ones_like(a0) # 2*10\n",
    "        for j in reversed(range(i+1,timestep+1)):      \n",
    "            da_dtanh     = (1 - np.square(activation[:,:,j]))          # (2*10)\n",
    "            da__da_prev  = da__da_prev * np.dot(Waa , da_dtanh)        # (2*10)\n",
    "        \n",
    "        da_dWax   =  np.dot(xt[:,:,i] , (1 - np.square(activation[:,:,i])).T) # (3*2)\n",
    "        \n",
    "                            # 3*2              2*10     10*2\n",
    "        dL_dWax  +=  np.dot(da_dWax , np.dot(dL_da , da__da_prev.T))       # (3*2)\n",
    "            \n",
    "    # dL_da0 = dL_da * da__da_prev\n",
    "    dL_da = np.dot(Way, (y_cap_ - y_))            # (2*10)\n",
    "    da_da_prev = 1\n",
    "    for i in reversed(range(timestep+1)): # 0 1 2 3\n",
    "        da_dtanh     = (1 - np.square(activation[:,:,i]))          # (2*10)\n",
    "        da__da_prev  = da__da_prev * np.dot(Waa , da_dtanh)        # (2*10)\n",
    "    \n",
    "    dL_da0 = dL_da * da__da_prev                                   # (2*10)\n",
    "    #dL_dby = dL_ds * ds_dby\n",
    "    #          (1*10) - (1*10)     \n",
    "    dL_dby = (y_cap_ - y_).sum(axis = 1).reshape(-1,1)  #   (1*1)\n",
    "                        \n",
    "    #dL_dba = dL_da * da_dba\n",
    "    #       (2*10)       (2*1) (1*10)-(1*10) \n",
    "    dL_dba = (a_ * np.dot(Way, (y_cap_ - y_))).sum(axis = 1 ).reshape(-1,1)   # (2*10)\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dL_da0\" :dL_da0,\n",
    "                 \"dL_dWaa\": dL_dWaa, \n",
    "                 \"dL_dWax\": dL_dWax,\n",
    "                 \"dL_dWay\": dL_dWay,\n",
    "                 \"dL_dba\" : dL_dba,\n",
    "                 \"dL_dby\" : dL_dby\n",
    "                 }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(a0, activations, y_caps, y1_ohe, parameters,xt):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da     -- Upstream gradients of all hidden states, of shape (batch_size, vocab_size, timestep)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                    dx  -- Gradient w.r.t. the input data, numpy-array of shape (batch_size, vocab_size, timestep)\n",
    "                    da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (hidden_unit, vocab_size)\n",
    "                    dWax-- Gradient w.r.t the input's weight matrix, numpy-array of shape (batch_size, hidden_unit)\n",
    "                    dWaa-- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (hidden_unit,hidden_unit)\n",
    "                    dba -- Gradient w.r.t the bias, of shape (hidden_unit, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, vocab_size, timestep =  y_caps.shape\n",
    "    hidden_unit = activations.shape[0]\n",
    "    \n",
    "    dWax = 0\n",
    "    dWay = 0\n",
    "    dWaa = 0\n",
    "    dba  = 0\n",
    "    da0  = 0\n",
    "    dby  = 0\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    \n",
    "    for t in range(timestep):\n",
    "        \n",
    "        gradient = rnn_cell_backward(t, y_caps, y1_ohe, activations, parameters,xt,a0)\n",
    "        \n",
    "        dL_da0      = gradient[\"dL_da0\"]\n",
    "        dL_dWaa     = gradient[\"dL_dWaa\"]\n",
    "        dL_dWax     = gradient[\"dL_dWax\"]\n",
    "        dL_dWay     = gradient[\"dL_dWay\"]   \n",
    "        dL_dba      = gradient[\"dL_dba\"]\n",
    "        dL_dby      = gradient[\"dL_dby\"]\n",
    "        \n",
    "        da0  += dL_da0\n",
    "        dWaa += dL_dWaa\n",
    "        dWax += dL_dWax\n",
    "        dWay += dL_dWay\n",
    "        dba  += dL_dba\n",
    "        dby  += dL_dby\n",
    "    \n",
    "    gradients = {\"da0\": da0, \"dWaa\": dWaa, \"dWax\": dWax, \"dWay\": dWay,\"dba\": dba,'dby':dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def crossEntropy(y_predict,y_actul):\n",
    "    cost = []\n",
    "    for i in range(y_predict.shape[2]): # 4\n",
    "        y_actual = y_actul[:,:,i]\n",
    "        y_pred   = y_caps[:,:,i]\n",
    "        cost.append(-np.sum(y_actual * np.log(y_pred)))\n",
    "    return np.mean(cost,axis=0)\n",
    "\n",
    "# one hot encoding\n",
    "def one_hot_encode(*x):\n",
    "    one = np.zeros([3,10,4])\n",
    "    for i1,i2 in enumerate(x):\n",
    "        for j1,j2 in enumerate(i2):\n",
    "            one[j1,:,i1] = one_hot_vector_vocab[char2idx[j2]].reshape(1,-1)\n",
    "    return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost:  8.714958911217202\n",
      "Final cost:  8.490240889190716\n",
      "Final cost:  8.27790018110784\n",
      "Final cost:  8.079808636054477\n",
      "Final cost:  7.898599061537567\n",
      "Final cost:  7.736670997966742\n",
      "Final cost:  7.594902332731643\n",
      "Final cost:  7.472442930617496\n",
      "Final cost:  7.367491931192761\n",
      "Final cost:  7.27782289907985\n",
      "Final cost:  7.201132485366464\n",
      "Final cost:  7.1353404646450596\n",
      "Final cost:  7.0787102246388445\n",
      "Final cost:  7.02983271713319\n",
      "Final cost:  6.9875646567829275\n",
      "Final cost:  6.950969580991405\n",
      "Final cost:  6.919275004379194\n",
      "Final cost:  6.891842765219503\n",
      "Final cost:  6.8681453865151525\n",
      "Final cost:  6.847742816683187\n",
      "Final cost:  6.830257622910905\n",
      "Final cost:  6.815350371468469\n",
      "Final cost:  6.802699224924156\n",
      "Final cost:  6.791987771989628\n",
      "Final cost:  6.782902518754906\n",
      "Final cost:  6.775141694133786\n",
      "Final cost:  6.768467670618382\n",
      "Final cost:  6.7628294613018785\n",
      "Final cost:  6.757441605123861\n",
      "Final cost:  6.748105778352948\n",
      "Final cost:  6.732556416168695\n",
      "Final cost:  6.713795340851124\n",
      "Final cost:  6.69414232088659\n",
      "Final cost:  6.674438029863387\n",
      "Final cost:  6.654946116350649\n",
      "Final cost:  6.635731902858317\n",
      "Final cost:  6.616787239895244\n",
      "Final cost:  6.598083536039304\n",
      "Final cost:  6.5795920926501195\n",
      "Final cost:  6.56128546171133\n",
      "Final cost:  6.54313494056326\n",
      "Final cost:  6.525110747242949\n",
      "Final cost:  6.5071844205945215\n",
      "Final cost:  6.489331649300894\n",
      "Final cost:  6.471534467532271\n",
      "Final cost:  6.453782473466633\n",
      "Final cost:  6.436073103612829\n",
      "Final cost:  6.418411158232502\n",
      "Final cost:  6.400807832646637\n",
      "Final cost:  6.38327951072385\n",
      "Final cost:  6.3658465331873675\n",
      "Final cost:  6.3485320755988015\n",
      "Final cost:  6.331361180898437\n",
      "Final cost:  6.314359918138941\n",
      "Final cost:  6.2975546047201405\n",
      "Final cost:  6.280971036339017\n",
      "Final cost:  6.264633697653798\n",
      "Final cost:  6.24856494991757\n",
      "Final cost:  6.232784196596217\n",
      "Final cost:  6.217307032225607\n",
      "Final cost:  6.202144429873535\n",
      "Final cost:  6.187302170590815\n",
      "Final cost:  6.172780990524487\n",
      "Final cost:  6.158578296245365\n",
      "Final cost:  6.144692710578883\n",
      "Final cost:  6.131133084840078\n",
      "Final cost:  6.117933878109746\n",
      "Final cost:  6.105178722932342\n",
      "Final cost:  6.093032538250458\n",
      "Final cost:  6.081776341100021\n",
      "Final cost:  6.071818964666631\n",
      "Final cost:  6.063610467540929\n",
      "Final cost:  6.057322502227089\n",
      "Final cost:  6.052316302073898\n",
      "Final cost:  6.047050283913896\n",
      "Final cost:  6.039997075326425\n",
      "Final cost:  6.0304856664964\n",
      "Final cost:  6.018560004677707\n",
      "Final cost:  6.004505612396597\n",
      "Final cost:  5.988581284710062\n",
      "Final cost:  5.97094695770777\n",
      "Final cost:  5.9516774594720125\n",
      "Final cost:  5.930801740755055\n",
      "Final cost:  5.908345509873255\n",
      "Final cost:  5.884370636754517\n",
      "Final cost:  5.859008619285051\n",
      "Final cost:  5.832485250499683\n",
      "Final cost:  5.805133077472406\n",
      "Final cost:  5.777389071106599\n",
      "Final cost:  5.749777558728063\n",
      "Final cost:  5.7228820583647675\n",
      "Final cost:  5.697312522902145\n",
      "Final cost:  5.673675227113957\n",
      "Final cost:  5.652550887736767\n",
      "Final cost:  5.63448359257603\n",
      "Final cost:  5.619980098784239\n",
      "Final cost:  5.609516927043897\n",
      "Final cost:  5.603551559109383\n",
      "Final cost:  5.602533553660808\n",
      "Final cost:  5.606910971116687\n",
      "Final cost:  5.617126651567847\n",
      "Final cost:  5.633597344964397\n",
      "Final cost:  5.656666594944399\n",
      "Final cost:  5.686520740830164\n",
      "Final cost:  5.723059759386146\n",
      "Final cost:  5.765728223043379\n",
      "Final cost:  5.8133464534806985\n",
      "Final cost:  5.864036616496719\n",
      "Final cost:  5.915369770733435\n",
      "Final cost:  5.964784873936438\n",
      "Final cost:  6.010139839713033\n",
      "Final cost:  6.050118625910533\n",
      "Final cost:  6.08431156341065\n",
      "Final cost:  6.113015202490022\n",
      "Final cost:  6.136928428062648\n",
      "Final cost:  6.1568893990320674\n",
      "Final cost:  6.173706761356882\n",
      "Final cost:  6.188077293448982\n",
      "Final cost:  6.200563023009344\n",
      "Final cost:  6.211603011396066\n",
      "Final cost:  6.221541539630248\n",
      "Final cost:  6.230659691511214\n",
      "Final cost:  6.2392016263930605\n",
      "Final cost:  6.247391002308923\n",
      "Final cost:  6.255436562516074\n",
      "Final cost:  6.263528050508447\n",
      "Final cost:  6.2718244870647775\n",
      "Final cost:  6.280437316315681\n",
      "Final cost:  6.289411725370015\n",
      "Final cost:  6.29871037400803\n",
      "Final cost:  6.308203879513995\n",
      "Final cost:  6.317671184667049\n",
      "Final cost:  6.326811402546847\n",
      "Final cost:  6.335268120107171\n",
      "Final cost:  6.342666453739522\n",
      "Final cost:  6.348660037733943\n",
      "Final cost:  6.3529798193725835\n",
      "Final cost:  6.3554727092449514\n",
      "Final cost:  6.356119336593722\n",
      "Final cost:  6.355026506420332\n",
      "Final cost:  6.352398190754991\n",
      "Final cost:  6.3484948869641125\n",
      "Final cost:  6.343592613194316\n",
      "Final cost:  6.337950124985661\n",
      "Final cost:  6.33178835668085\n",
      "Final cost:  6.325281916422462\n",
      "Final cost:  6.3185599387265325\n",
      "Final cost:  6.311712802666584\n",
      "Final cost:  6.304801582149203\n",
      "Final cost:  6.2978679394938215\n",
      "Final cost:  6.290943042701768\n",
      "Final cost:  6.284054768039205\n",
      "Final cost:  6.277232900822698\n",
      "Final cost:  6.270512306565315\n",
      "Final cost:  6.263934172027204\n",
      "Final cost:  6.25754546759237\n",
      "Final cost:  6.251396804456377\n",
      "Final cost:  6.245538888550209\n",
      "Final cost:  6.240017837380265\n",
      "Final cost:  6.234869748137676\n",
      "Final cost:  6.2301150914568355\n",
      "Final cost:  6.225753725179841\n",
      "Final cost:  6.221761488618306\n",
      "Final cost:  6.218089308866897\n",
      "Final cost:  6.214665400032507\n",
      "Final cost:  6.21140047434726\n",
      "Final cost:  6.208195133668493\n",
      "Final cost:  6.2049481097689565\n",
      "Final cost:  6.201563994132599\n",
      "Final cost:  6.1979594900309785\n",
      "Final cost:  6.194067773393856\n",
      "Final cost:  6.189841006551881\n",
      "Final cost:  6.1852512972528615\n",
      "Final cost:  6.18029046901481\n",
      "Final cost:  6.174969010649464\n",
      "Final cost:  6.169314596822372\n",
      "Final cost:  6.163370668904983\n",
      "Final cost:  6.157195753054782\n",
      "Final cost:  6.150864492406104\n",
      "Final cost:  6.144471864688217\n",
      "Final cost:  6.138142952730622\n",
      "Final cost:  6.13205235446646\n",
      "Final cost:  6.12646065267355\n",
      "Final cost:  6.121781717018486\n",
      "Final cost:  6.118705858574186\n",
      "Final cost:  6.1184172751962995\n",
      "Final cost:  6.1229073288687115\n",
      "Final cost:  6.1348730426941795\n",
      "Final cost:  6.1535154759087725\n",
      "Final cost:  6.160090224352267\n",
      "Final cost:  6.139560015787335\n",
      "Final cost:  6.114718514684737\n",
      "Final cost:  6.093124675674237\n",
      "Final cost:  6.073517794075345\n",
      "Final cost:  6.055064832383501\n",
      "Final cost:  6.037327024632884\n",
      "Final cost:  6.020149175713767\n",
      "Final cost:  6.004433370754322\n",
      "Final cost:  5.992089279180723\n",
      "Final cost:  5.982252848295612\n",
      "Final cost:  5.972933254917359\n",
      "Final cost:  5.963446570048519\n",
      "Final cost:  5.953698873614512\n",
      "Final cost:  5.9437343823345286\n",
      "Final cost:  5.933642039540179\n",
      "Final cost:  5.923558765374528\n",
      "Final cost:  5.913698722062302\n",
      "Final cost:  5.904395025072042\n",
      "Final cost:  5.896150114423985\n",
      "Final cost:  5.889684438186535\n",
      "Final cost:  5.885950707487439\n",
      "Final cost:  5.886022642925994\n",
      "Final cost:  5.890647254832212\n",
      "Final cost:  5.8992521889805865\n",
      "Final cost:  5.9092877560999115\n",
      "Final cost:  5.918655268923349\n",
      "Final cost:  5.929425757363825\n",
      "Final cost:  5.945882811647106\n",
      "Final cost:  5.970959201817249\n",
      "Final cost:  6.005951715216478\n",
      "Final cost:  6.050176970239671\n",
      "Final cost:  6.0996159039591795\n",
      "Final cost:  6.1499997562015105\n",
      "Final cost:  6.203222034398735\n",
      "Final cost:  6.260876975823145\n",
      "Final cost:  6.322501849303638\n",
      "Final cost:  6.394879346965\n",
      "Final cost:  6.488601324817383\n",
      "Final cost:  6.608901280480399\n",
      "Final cost:  6.749159141259295\n",
      "Final cost:  6.896566634422877\n",
      "Final cost:  7.038414132685757\n",
      "Final cost:  7.163787203146484\n",
      "Final cost:  7.2657875751411805\n",
      "Final cost:  7.3422955089887365\n",
      "Final cost:  7.3942674610824515\n",
      "Final cost:  7.423444485108165\n",
      "Final cost:  7.430893428408586\n",
      "Final cost:  7.417261282903523\n",
      "Final cost:  7.382496442004433\n",
      "Final cost:  7.327596167824426\n",
      "Final cost:  7.251879556350032\n",
      "Final cost:  7.147814157768254\n",
      "Final cost:  7.021800191365852\n",
      "Final cost:  6.894152080704053\n",
      "Final cost:  6.792299117229767\n",
      "Final cost:  6.703414907890333\n",
      "Final cost:  6.626255199639543\n",
      "Final cost:  6.571024280328591\n",
      "Final cost:  6.525142115248455\n",
      "Final cost:  6.48180026121358\n",
      "Final cost:  6.43938486619005\n",
      "Final cost:  6.398050342258735\n",
      "Final cost:  6.357620435248703\n",
      "Final cost:  6.317590708806344\n",
      "Final cost:  6.277230105517517\n",
      "Final cost:  6.2355973470133685\n",
      "Final cost:  6.191769245992392\n",
      "Final cost:  6.146216474539663\n",
      "Final cost:  6.102733778597554\n",
      "Final cost:  6.063743732426576\n",
      "Final cost:  6.026816356987329\n",
      "Final cost:  5.990295038548815\n",
      "Final cost:  5.955184330672471\n",
      "Final cost:  5.923036562809709\n",
      "Final cost:  5.885840592577216\n",
      "Final cost:  5.836270994804078\n",
      "Final cost:  5.777795984395068\n",
      "Final cost:  5.71449231202767\n",
      "Final cost:  5.65056886155518\n",
      "Final cost:  5.593982154129465\n",
      "Final cost:  5.549460160061495\n",
      "Final cost:  5.510848981515919\n",
      "Final cost:  5.475757669005455\n",
      "Final cost:  5.446361265860338\n",
      "Final cost:  5.422656093922218\n",
      "Final cost:  5.403758979283386\n",
      "Final cost:  5.388608014770881\n",
      "Final cost:  5.375886867001407\n",
      "Final cost:  5.363972949967492\n",
      "Final cost:  5.351190207624276\n",
      "Final cost:  5.336224233137649\n",
      "Final cost:  5.318340268277089\n",
      "Final cost:  5.297337993234983\n",
      "Final cost:  5.27341418109949\n",
      "Final cost:  5.2466304433800826\n",
      "Final cost:  5.215826711393708\n",
      "Final cost:  5.179606990780586\n",
      "Final cost:  5.139655432775507\n",
      "Final cost:  5.100950411643316\n",
      "Final cost:  5.068949886069589\n",
      "Final cost:  5.047856561519553\n",
      "Final cost:  5.040613335345394\n",
      "Final cost:  5.049147640431176\n",
      "Final cost:  5.074216167853731\n",
      "Final cost:  5.115431732223158\n",
      "Final cost:  5.172024075359513\n",
      "Final cost:  5.242674070765966\n",
      "Final cost:  5.317459462324155\n",
      "Final cost:  5.354102373781759\n",
      "Final cost:  5.282727571276225\n",
      "Final cost:  5.119296366815378\n",
      "Final cost:  5.058159875256997\n",
      "Final cost:  5.027169987585097\n",
      "Final cost:  5.004697399421697\n",
      "Final cost:  4.987243732220724\n",
      "Final cost:  4.973933182531549\n",
      "Final cost:  4.964712508705593\n",
      "Final cost:  4.959888627291097\n",
      "Final cost:  4.959966142567902\n",
      "Final cost:  4.965575816622467\n",
      "Final cost:  4.9774176426196535\n",
      "Final cost:  4.996108617664945\n",
      "Final cost:  5.021255830836475\n",
      "Final cost:  5.042779804747337\n",
      "Final cost:  4.959745957129526\n",
      "Final cost:  4.969036415265789\n",
      "Final cost:  4.948811017947688\n",
      "Final cost:  4.927338096237218\n",
      "Final cost:  4.904566157412045\n",
      "Final cost:  4.880370310670214\n",
      "Final cost:  4.854589603415687\n",
      "Final cost:  4.827053149506796\n",
      "Final cost:  4.797605360683215\n",
      "Final cost:  4.766135536996023\n",
      "Final cost:  4.732614072405495\n",
      "Final cost:  4.69713408673086\n",
      "Final cost:  4.659952612952736\n",
      "Final cost:  4.62152017865219\n",
      "Final cost:  4.582483391199773\n",
      "Final cost:  4.543643860285021\n",
      "Final cost:  4.505862325237006\n",
      "Final cost:  4.469917098582632\n",
      "Final cost:  4.436358487307583\n",
      "Final cost:  4.405412472076158\n",
      "Final cost:  4.3769420051881305\n",
      "Final cost:  4.350371835965877\n",
      "Final cost:  4.324230370079163\n",
      "Final cost:  4.29358951648606\n",
      "Final cost:  4.2348004706347995\n",
      "Final cost:  4.137670403838969\n",
      "Final cost:  4.137500026413087\n",
      "Final cost:  4.135854680569881\n",
      "Final cost:  4.129822735383989\n",
      "Final cost:  4.121717299475312\n",
      "Final cost:  4.112702249736653\n",
      "Final cost:  4.103456640518507\n",
      "Final cost:  4.094472840918798\n",
      "Final cost:  4.086176713608405\n",
      "Final cost:  4.07897024900255\n",
      "Final cost:  4.07322850617227\n",
      "Final cost:  4.069260504241524\n",
      "Final cost:  4.067247384457355\n",
      "Final cost:  4.0671907330593715\n",
      "Final cost:  4.068913733505158\n",
      "Final cost:  4.0721277156735605\n",
      "Final cost:  4.076525002505246\n",
      "Final cost:  4.081845586891659\n",
      "Final cost:  4.087899202118514\n",
      "Final cost:  4.0945561468359335\n",
      "Final cost:  4.101724914401244\n",
      "Final cost:  4.109327008692405\n",
      "Final cost:  4.117273958071584\n",
      "Final cost:  4.125450089502149\n",
      "Final cost:  4.133704195823794\n",
      "Final cost:  4.141851876311826\n",
      "Final cost:  4.1496876955290265\n",
      "Final cost:  4.157003310033867\n",
      "Final cost:  4.163605951334993\n",
      "Final cost:  4.169332216834471\n",
      "Final cost:  4.1740546494060595\n",
      "Final cost:  4.177681578776197\n",
      "Final cost:  4.180152743488749\n",
      "Final cost:  4.181433869705412\n",
      "Final cost:  4.181513080293798\n",
      "Final cost:  4.180401419495574\n",
      "Final cost:  4.178139297033917\n",
      "Final cost:  4.174810018232829\n",
      "Final cost:  4.1705592537407945\n",
      "Final cost:  4.165611730520231\n",
      "Final cost:  4.16026060486564\n",
      "Final cost:  4.154796652230693\n",
      "Final cost:  4.149401457738402\n",
      "Final cost:  4.144144754237793\n",
      "Final cost:  4.139140876532727\n",
      "Final cost:  4.134701023278724\n",
      "Final cost:  4.131545073586412\n",
      "Final cost:  4.1311743069085365\n",
      "Final cost:  4.136158199425547\n",
      "Final cost:  4.149837169590525\n",
      "Final cost:  4.175480744631368\n",
      "Final cost:  4.2131538725569495\n",
      "Final cost:  4.251773324818553\n",
      "Final cost:  4.299094838071423\n",
      "Final cost:  4.336356190053593\n",
      "Final cost:  4.414440503455354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost:  4.379834953367501\n",
      "Final cost:  4.410234393105546\n",
      "Final cost:  4.399040368988815\n",
      "Final cost:  4.392507947870941\n",
      "Final cost:  4.38226437157717\n",
      "Final cost:  4.376938102076\n",
      "Final cost:  4.369868585558799\n",
      "Final cost:  4.366245895180701\n",
      "Final cost:  4.3626397778518005\n",
      "Final cost:  4.360812944825141\n",
      "Final cost:  4.359689483415532\n",
      "Final cost:  4.359482977885808\n",
      "Final cost:  4.359926878566681\n",
      "Final cost:  4.360935916890213\n",
      "Final cost:  4.3623971101919015\n",
      "Final cost:  4.364212943276863\n",
      "Final cost:  4.366285445846884\n",
      "Final cost:  4.368520491557525\n",
      "Final cost:  4.370836460884493\n",
      "Final cost:  4.373171735831868\n",
      "Final cost:  4.37548412041868\n",
      "Final cost:  4.377742872713763\n",
      "Final cost:  4.379919582949105\n",
      "Final cost:  4.381983356326275\n",
      "Final cost:  4.383901417896263\n",
      "Final cost:  4.385642764732017\n",
      "Final cost:  4.3871816924227955\n",
      "Final cost:  4.388499311329548\n",
      "Final cost:  4.389582952336875\n",
      "Final cost:  4.39042440917417\n",
      "Final cost:  4.3910180359257645\n",
      "Final cost:  4.391359271020101\n",
      "Final cost:  4.391443695844955\n",
      "Final cost:  4.391266475818551\n",
      "Final cost:  4.390821959991699\n",
      "Final cost:  4.390103245176778\n",
      "Final cost:  4.389101577600879\n",
      "Final cost:  4.387805545159205\n",
      "Final cost:  4.386200104049731\n",
      "Final cost:  4.38426557112378\n",
      "Final cost:  4.381976734894694\n",
      "Final cost:  4.379302060668114\n",
      "Final cost:  4.376202425082701\n",
      "Final cost:  4.372627832245179\n",
      "Final cost:  4.368509186634908\n",
      "Final cost:  4.3637403046839545\n",
      "Final cost:  4.358142022355395\n",
      "Final cost:  4.35139417556489\n",
      "Final cost:  4.342919724970807\n",
      "Final cost:  4.331792706454452\n",
      "Final cost:  4.317440024528361\n",
      "Final cost:  4.304025135327569\n",
      "Final cost:  4.299576199587698\n",
      "Final cost:  4.293617260194745\n",
      "Final cost:  4.288661011546413\n",
      "Final cost:  4.282694938640828\n",
      "Final cost:  4.276719953995038\n",
      "Final cost:  4.270024545944322\n",
      "Final cost:  4.262896319147052\n",
      "Final cost:  4.254974886050991\n",
      "Final cost:  4.246276207128619\n",
      "Final cost:  4.236557942893313\n",
      "Final cost:  4.225810754393298\n",
      "Final cost:  4.214049495710654\n",
      "Final cost:  4.20169934430017\n",
      "Final cost:  4.189766870683334\n",
      "Final cost:  4.179791160700588\n",
      "Final cost:  4.1723650826297884\n",
      "Final cost:  4.16680474313589\n",
      "Final cost:  4.162180439368598\n",
      "Final cost:  4.15794450084466\n",
      "Final cost:  4.153834257063943\n",
      "Final cost:  4.149768197429895\n",
      "Final cost:  4.145715124866621\n",
      "Final cost:  4.14168522103239\n",
      "Final cost:  4.1376797923723\n",
      "Final cost:  4.1337146324905465\n",
      "Final cost:  4.1297926357590296\n",
      "Final cost:  4.125930110345479\n",
      "Final cost:  4.122137340582473\n",
      "Final cost:  4.1184415913699945\n",
      "Final cost:  4.114871951212535\n",
      "Final cost:  4.1114779136261035\n",
      "Final cost:  4.108318216369874\n",
      "Final cost:  4.105476733975342\n",
      "Final cost:  4.103057502897857\n",
      "Final cost:  4.101201848769929\n",
      "Final cost:  4.10009323260049\n",
      "Final cost:  4.099980702005537\n",
      "Final cost:  4.1011975154398375\n",
      "Final cost:  4.104195138519781\n",
      "Final cost:  4.109575761716488\n",
      "Final cost:  4.118131974742317\n",
      "Final cost:  4.1308762616111805\n",
      "Final cost:  4.149055099382341\n",
      "Final cost:  4.17412686744229\n",
      "Final cost:  4.207694913520231\n",
      "Final cost:  4.2513891311792396\n",
      "Final cost:  4.306697036509862\n",
      "Final cost:  4.374706843104848\n",
      "Final cost:  4.455674228357137\n",
      "Final cost:  4.548621715264787\n",
      "Final cost:  4.652774475833107\n",
      "Final cost:  4.756318225574807\n"
     ]
    }
   ],
   "source": [
    "x = one_hot_encode(x1,x2,x3,x4) # (3*10*4)\n",
    "y = one_hot_encode(y1,y2,y3,y4) # (3*10*4)\n",
    "np.random.seed(1)\n",
    "Wax = np.random.randn(3,5)   # (5*2)    ==> batch_size    * hidden_neuron\n",
    "\n",
    "a0  = np.random.randn(5,10)  # (2*10)   ==> hidden_neuron * vocab_size\n",
    "Waa = np.random.randn(5,5)   # (2*2)    ==> hidden_neuron * hidden_neuron\n",
    "ba  = np.random.randn(5,1)   # (2*1)    ==> hidden nueron * 1\n",
    "\n",
    "Way = np.random.randn(5,3)   # (2*3)    ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by  = np.random.randn(3,1)   # (3*1)    ==> output_neuron * 1\n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "#######################\n",
    "# stochastic gradient #\n",
    "#######################\n",
    "alpha = 0.01\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "for i in range(500):\n",
    "    a0, activations, y_caps = rnn_forward(x, a0, parameters)\n",
    "    gradients               = rnn_backward(a0, activations, y_caps, y, parameters,x)\n",
    "    print(\"Final cost: \",crossEntropy(y_caps,y))\n",
    "    parameters['Waa'] -= (alpha * gradients['dWaa'])\n",
    "    parameters['Wax'] -= (alpha * gradients['dWax'])\n",
    "    parameters['Way'] -= (alpha * gradients['dWay'])\n",
    "    parameters['ba']  -= (alpha * gradients['dba'])\n",
    "    parameters['by']  -= (alpha * gradients['dby'])\n",
    "    parameters['by']  -= (alpha * gradients['dby'])\n",
    "    a0                -= (alpha * gradients['da0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 1 4]\n",
      "[6 6 5]\n",
      "[9 8 2]\n",
      "[3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(x[:,:,0],axis=1))\n",
    "print(np.argmax(x[:,:,1],axis=1))\n",
    "print(np.argmax(x[:,:,2],axis=1))\n",
    "print(np.argmax(x[:,:,3],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 5]\n",
      "[9 8 2]\n",
      "[3 3 3]\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(y[:,:,0],axis=1))\n",
    "print(np.argmax(y[:,:,1],axis=1))\n",
    "print(np.argmax(y[:,:,2],axis=1))\n",
    "print(np.argmax(y[:,:,3],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6]\n",
      "[9 9 2]\n",
      "[6 6 7]\n",
      "[8 8 5]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(y_caps[:,:,0],axis=1))\n",
    "print(np.argmax(y_caps[:,:,1],axis=1))\n",
    "print(np.argmax(y_caps[:,:,2],axis=1))\n",
    "print(np.argmax(y_caps[:,:,3],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "(3, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y[:,:,0].shape)\n",
    "print(y_caps[:,:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual_input\t\tActual_expected\t\t\tPredicted\n",
      "mango,banana,hair,\t\tis,is,has,\t\tis,is,is,\n",
      "is,is,has,\t\tyellow,pink,black,\t\tblack,black,black,\n",
      "yellow,pink,black,\t\tcolor,color,color,\t\tmango,mango,mango,\n",
      "color,color,color,\t\t<end>,<end>,<end>,\t\thas,has,has,\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "#     ACCURACY     #\n",
    "####################\n",
    "print(\"Actual_input\\t\\tActual_expected\\t\\t\\tPredicted\")\n",
    "for i in range(4):\n",
    "    y_ = np.argmax(y[:,:,i],axis=1)\n",
    "    y_cap = np.argmax(y_caps[:,:,i],axis=1)\n",
    "    x_ = np.argmax(x[:,:,i],axis=1)\n",
    "    for p in range(3):\n",
    "        print(idx2char[x_[p]],end=',')\n",
    "    print('\\t\\t',end='')\n",
    "    for j in range(3):\n",
    "        print(idx2char[y_[j]],end=',')\n",
    "    print('\\t\\t',end='')\n",
    "    for k in range(3):\n",
    "        print(idx2char[y_cap[j]],end=',')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
