{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network \n",
    "\n",
    "A neural network usually takes an independent variable X (or a set of independent variables ) and a dependent variable y then it learns the mapping between X and y (we call this Training), Once training is done , we give a new independent variable to predict the dependent variable.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. \n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
    "\n",
    "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example input.\n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
    "    - Example: $x^{\\langle t \\rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\\langle t \\rangle}$ is the input at the $t^{th}$ timestep of example $i$.\n",
    "    \n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Forward propagation for the basic Recurrent Neural Network\n",
    "\n",
    "Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can implement an RNN: \n",
    "\n",
    "**Steps**:\n",
    "1. Implement the calculations needed for one time-step of the RNN.\n",
    "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. \n",
    "$T_x$ is just a length of your sequence\n",
    "\n",
    "Let's go!\n",
    "\n",
    "## 1.1 - RNN cell\n",
    "\n",
    "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided you a function: `softmax`.\n",
    "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in cache\n",
    "4. Return $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ and cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   <end>  banana  black  color  hair  has  is  mango  pink  yellow\n",
      "0      1       0      0      0     0    0   0      0     0       0\n",
      "1      0       1      0      0     0    0   0      0     0       0\n",
      "2      0       0      1      0     0    0   0      0     0       0\n",
      "3      0       0      0      1     0    0   0      0     0       0\n",
      "4      0       0      0      0     1    0   0      0     0       0\n",
      "5      0       0      0      0     0    1   0      0     0       0\n",
      "6      0       0      0      0     0    0   1      0     0       0\n",
      "7      0       0      0      0     0    0   0      1     0       0\n",
      "8      0       0      0      0     0    0   0      0     1       0\n",
      "9      0       0      0      0     0    0   0      0     0       1\n"
     ]
    }
   ],
   "source": [
    "# lets we have four sentences \n",
    "s1 = 'mango is yellow color'\n",
    "s2 = 'banana is pink color'\n",
    "s3 = 'hair has black color'\n",
    "\n",
    "# Now what is and how the matrices shapes define\n",
    "# create vocab\n",
    "l = [*s1.split(),*s2.split(),*s3.split()]\n",
    "l.append('<end>')\n",
    "vocab = sorted(set(l))\n",
    "one_hot_vector_vocab = np.array(pd.get_dummies(vocab))\n",
    "print(pd.get_dummies(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create two list which stores [character_to_index] values and [index_to_character] values\n",
    "\n",
    "char2idx = {value:index for index,value in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max length of time step\n",
    "\n",
    "seq_length = max(len(s1.split()),len(s2.split()),len(s3.split()))\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Input data for single batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single batch  (1*10*4) or (10*4) loop over 4 i.e loop (1*10) (1*10) (1*10) (1*10)\n",
    "\n",
    "##############################################################\n",
    "# input  #       shape      #   target   #       shape       #\n",
    "##############################################################\n",
    "# mango  (1*10) onehotvector     is      (1*10) onehotvector #\n",
    "# is     (1*10) onehotvector    yellow   (1*10) onehotvector #\n",
    "# yellow (1*10) onehotvector    color    (1*10) onehotvector #\n",
    "# color  (1*10) onehotvector    <end>    (1*10) onehotvector #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 9, 3]\n",
      "(10, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right now we pass single sentence or single batch so the input size will become \n",
    "#(1*10*5) batchsize * vocab_size * sequence_size or timestep \n",
    "print([char2idx[char] for char in s1.split()])\n",
    "print(one_hot_vector_vocab[[7,6,9,3]].T.shape)\n",
    "one_hot_vector_vocab[[7,6,9,3]].T  # mango is yellow color OH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mango', 'is', 'yellow', 'color'], dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input array for single batch\n",
    "np.array(s1.split()) # 4 words each represent in One Hot Encoding  dim = (10*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(10, 4)\n",
      "(10, 4)\n",
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare input data for single batch pass\n",
    "x1 = one_hot_vector_vocab[[char2idx[char] for char in s1.split()]].T #s1 = ' mango is yellow color '\n",
    "x2 = one_hot_vector_vocab[[char2idx[char] for char in s2.split()]].T #s2 = ' banana is pink color  '\n",
    "x3 = one_hot_vector_vocab[[char2idx[char] for char in s3.split()]].T #s3 = ' hair has black color  '\n",
    "print(x1.shape) # these are\n",
    "print(x2.shape) # three\n",
    "print(x3.shape) # different samples\n",
    "\n",
    "print(x1[:,0].shape) # 0th time step means we are passing mango as an input\n",
    "x1[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing target variable for single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'yellow' 'color' '<end>']\n",
      "['is' 'pink' 'color' '<end>']\n",
      "['has' 'black' 'color' '<end>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 9, 3, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = s1.split()[1:] # why I take from 1st index? bcz first input word predict 2nd so we expect 2 word from output\n",
    "y2 = s2.split()[1:]\n",
    "y3 = s3.split()[1:]\n",
    "y1.append('<end>') ; y2.append('<end>') ; y3.append('<end>') ;\n",
    "y1 = np.array(y1)\n",
    "y2 = np.array(y2)\n",
    "y3 = np.array(y3)\n",
    "print(y1) # these are\n",
    "print(y2) # three \n",
    "print(y3) # different samples\n",
    "[char2idx[i] for i in y1] # this is expected targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 3, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = np.array([char2idx[i] for i in y1]) # this is expected targets\n",
    "y2 = np.array([char2idx[i] for i in y2]) # this is expected targets\n",
    "y3 = np.array([char2idx[i] for i in y3]) # this is expected targets\n",
    "y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Single timestep feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    return np.exp(a)/np.exp(a).sum(axis=1)\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt         --  (batch_unit_size , vocab_size)\n",
    "    a_prev     --  (hidden_unit_size, vocab_size)\n",
    "\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- (batch_unit_size , hidden_unit_size)\n",
    "                        Waa -- (hidden_unit_size, hidden_unit_size)\n",
    "                        Wya -- (hidden_unit_size, output_unit_size)\n",
    "                        ba --  (hidden_unit_size, 1)\n",
    "                        by --  (output_unit_size, 1)\n",
    "    Returns:\n",
    "    a_next  -- (hidden_unit, vocab_size)\n",
    "    yt_pred -- (output_unit, vocab_size)\n",
    "    cache   -- (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Way = parameters[\"Way\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa.T, a_prev) + np.dot(Wax.T, xt) + ba) # current activation bias\n",
    "    y_cap = softmax(np.dot(Way.T, a_next) + by)  # output of current time step\n",
    "\n",
    "    return (a_prev, a_next, y_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:,0] # mango\n",
    "x1[:,1] # is \n",
    "x1[:,2] # yellow\n",
    "x1[:,3] # color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           Wax.T  *  Xt               Way.T  * a_current\n",
    "# (1*10)    (2*1)  * (1*10)             (1*2) *  (2*10)\n",
    "# mango ---------------------> (2*10) --------------------->  is = (1*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next =  (2, 10)\n",
      "a_next.shape =  (2, 10)\n",
      "y_cap.shape =  (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# we create 1 hidden layer with 2 neurons\n",
    "np.random.seed(1)\n",
    "xt     = x1[:,0].reshape(1,-1)  # (1*10)  ==> batch_size    * vocab_size \n",
    "Wax    = np.random.randn(1,2)   # (1*2)   ==> batch_size    * hidden_neuron\n",
    "\n",
    "a_prev = np.random.randn(2,10)  # (2*10)  ==> hidden_neuron * vocab_size\n",
    "Waa    = np.random.randn(2,2)   # (2*2)   ==> hidden_neuron * hidden_neuron\n",
    "ba     = np.random.randn(2,1)   # (2*1)   ==> hidden nueron * 1\n",
    "\n",
    "Way    = np.random.randn(2,1)   # (2*1)   ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by     = np.random.randn(1,1)   # (1*1)   ==> output_neuron * 1\n",
    "\n",
    "# In sentiment anlysis output_neron will be no. of labels \n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_prev, a_next, y_cap = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next = \", a_next.shape)\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"y_cap.shape = \", y_cap.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Multi timestep feedforward \n",
    "#### Don't think that it's a multi neuron like NN. It just a single neuron where activations are feed in loop \n",
    "\n",
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 4 time steps, then you will copy the RNN cell 4 times. Each cell takes as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    xt         --  (batch_unit_size , vocab_size , timestep)\n",
    "    a_prev     --  (hidden_unit_size, vocab_size)\n",
    "\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- (batch_unit_size , hidden_unit_size)\n",
    "                        Waa -- (hidden_unit_size, hidden_unit_size)\n",
    "                        Way -- (hidden_unit_size, output_unit_size)\n",
    "                        ba --  (hidden_unit_size, 1)\n",
    "                        by --  (output_unit_size, 1)\n",
    "    Returns:\n",
    "    a_next  -- (hidden_unit, vocab_size)\n",
    "    yt_pred -- (output_unit, vocab_size)\n",
    "    cache   -- (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []  # store outputs which we get using single timestep  [need during backpropagation]\n",
    "    \n",
    "    hidden_neuron , vocab_size         =  a_prev.shape\n",
    "    batch_size , vocab_size , timestep =  xt.shape\n",
    "    hidden_neuron, output_neuron       =  parameters[\"Way\"].shape\n",
    "    \n",
    "    activations = np.zeros([hidden_neuron,vocab_size,timestep])  # store activations during each timesteps\n",
    "    y_caps      = np.zeros([output_neuron,vocab_size,timestep])  # store output at each timesteps\n",
    "\n",
    "    a_next_t = a_prev  # why? you will get soon\n",
    "    a0 = 0\n",
    "    # loop over all time-steps\n",
    "    for t in range(timestep):\n",
    "        \n",
    "        a_prev_t, a_next_t, y_cap_t = rnn_cell_forward(xt[:,:,t], a_next_t, parameters)\n",
    "        if t == 0:\n",
    "            a0 = a_prev_t # store only first prev activation\n",
    "        activations[:,:,t]   = a_next_t\n",
    "        y_caps[:,:,t]        = y_cap_t\n",
    "    \n",
    "    return a0, activations, y_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations.shape =  (2, 10, 4)\n",
      "y_caps.shape      =  (1, 10, 4)\n",
      "a_prev            =  [[-0.52817175 -1.07296862  0.86540763 -2.3015387   1.74481176 -0.7612069\n",
      "   0.3190391  -0.24937038  1.46210794 -2.06014071]\n",
      " [-0.3224172  -0.38405435  1.13376944 -1.09989127 -0.17242821 -0.87785842\n",
      "   0.04221375  0.58281521 -1.10061918  1.14472371]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt     = x1.reshape(1,10,4)     # (1*10*4) ==> batch_size    * vocab_size * timestep \n",
    "Wax    = np.random.randn(1,2)   # (1*2)    ==> batch_size    * hidden_neuron\n",
    "\n",
    "a_prev = np.random.randn(2,10)  # (2*10)   ==> hidden_neuron * vocab_size\n",
    "Waa    = np.random.randn(2,2)   # (2*2)    ==> hidden_neuron * hidden_neuron\n",
    "ba     = np.random.randn(2,1)   # (2*1)    ==> hidden nueron * 1\n",
    "\n",
    "Way    = np.random.randn(2,1)   # (2*1)    ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by     = np.random.randn(1,1)   # (1*1)    ==> output_neuron * 1\n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a0, activations, y_caps = rnn_forward(xt, a_prev, parameters)\n",
    "print(\"activations.shape = \", activations.shape)\n",
    "print(\"y_caps.shape      = \", y_caps.shape)\n",
    "print(\"a_prev            = \", a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "<img src=\"images/costfunction.jpeg\" style=\"width:500;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Basic RNN  backward pass\n",
    "\n",
    "We will start by computing the backward pass for the basic RNN-cell.\n",
    "\n",
    "<img src=\"images/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 6**: RNN-cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculus. The chain-rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(timestep, y_caps, y, activation, parameters,xt,a0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache   -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx      -- Gradients of input data, of shape              (batch_size ,vocab_size, timestep)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape   (hidden_unit,vocab_size)\n",
    "                        dWax    -- Gradients of input-to-hidden weights, of shape (batch_size ,hidden_unit)\n",
    "                        dWaa    -- Gradients of hidden-to-hidden weights, of shape(hidden_unit,hidden_unit)\n",
    "                        dba     -- Gradients of bias vector, of shape             (hidden_unit, 1)\n",
    "    \"\"\"\n",
    "    Wax = parameters[\"Wax\"] #(1*2)\n",
    "    Waa = parameters[\"Waa\"] #(2*2)\n",
    "    Way = parameters[\"Way\"] #(2*1)\n",
    "    ba  = parameters[\"ba\"]  #(2*1)\n",
    "    by  = parameters[\"by\"]  #(1*1)\n",
    "    \n",
    "    y_cap_ = y_caps[:,:,timestep]      # current timestep y_pred\n",
    "    y_     = y[:,:,timestep]           # current timestep y_actual\n",
    "    a_     = activation[:,:,timestep]  # current timestep a_next\n",
    "    xt_    = xt[:,:,timestep]          # current timestep input\n",
    "    a0_    = a0                        # a_prev for first timestep\n",
    "    \n",
    "    # compute derivative Way w.r.t to Loss\n",
    "    #                (2*10) (1*10)-(1*10)\n",
    "    dL_dWay = np.dot( a_,  (y_cap_ - y_).T)  # (2*1) \n",
    "    \n",
    "    # dL_dWaa = dL_da * da__da_prev * da_da_prev__dWaa \n",
    "    dL_dWaa = 0\n",
    "    for i in range(timestep+1):  # t = 2    ===> 0 1 \n",
    "        #                    (2*1)  (1*10)-(1*10) \n",
    "        dL_da        = np.dot(Way, (y_cap_ - y_))   # (2*10)\n",
    "        \n",
    "        da__da_prev = 1\n",
    "        for j in reversed(range(i+1,timestep+1)): # t = 2  ===> 1   \n",
    "            #                               (2*10)\n",
    "            da_dtanh     = (1 - np.square(activation[:,:,j]))          # (2*10)\n",
    "            #                                  (2*2)  (2*10)   \n",
    "            da__da_prev  = da__da_prev * np.dot(Waa , da_dtanh)        # (2*10)\n",
    "            \n",
    "        if i == 0: \n",
    "    \n",
    "            #           (2*10)                    (2*10)  \n",
    "            da_dWaa   =  a0_   *  (1 - np.square(activation[:,:,i])) # (2*10)        \n",
    "        else:\n",
    "            #                          (2*10)                               (2*10)\n",
    "            da_dWaa   =  (1 - np.square(activation[:,:,i-1]))   *  (1 - np.square(activation[:,:,i])) # (2*10)\n",
    "            \n",
    "            #            (2*10)     (2*10)       (10*2)\n",
    "        dL_dWaa += np.dot(dL_da * da__da_prev , da_dWaa.T)  # (2*2)\n",
    "    \n",
    "    # dL_dWax = dL_da * da__da_prev * da_da_prev__dWax \n",
    "    dL_dWax = 0\n",
    "    for i in range(timestep+1):  # t = 1    ===> 0  \n",
    "        dL_da       = np.dot(Way, (y_cap_ - y_))   # (2*10)\n",
    "        da__da_prev = 1\n",
    "        for j in reversed(range(i+1,timestep+1)): # t = 1  ===>     \n",
    "            da_dtanh     = (1 - np.square(activation[:,:,j]))          # (2*10)\n",
    "            da__da_prev  = da__da_prev * np.dot(Waa , da_dtanh)        # (2*10)\n",
    "        \n",
    "        da_dWax   =  xt[:,:,i]   *  (1 - np.square(activation[:,:,i])) # (2*10)\n",
    "        \n",
    "        #            (2*10)    (2*10)      (2*10)\n",
    "        dL_dWax  +=  dL_da * da__da_prev * da_dWax     # (1*2)\n",
    "    \n",
    "    dL_dWax = dL_dWax.sum(axis=1).reshape(1,-1)\n",
    "    # dL_da0 = dL_da * da__da_prev\n",
    "    dL_da = np.dot(Way, (y_cap_ - y_))            # (2*10)\n",
    "    da_da_prev = 1\n",
    "    for i in reversed(range(timestep+1)): # 0 1 2 3\n",
    "        da_dtanh     = (1 - np.square(activation[:,:,i]))          # (2*10)\n",
    "        da__da_prev  = da__da_prev * np.dot(Waa , da_dtanh)        # (2*10)\n",
    "    \n",
    "    dL_da0 = dL_da * da__da_prev                                   # (2*10)\n",
    "    #dL_dby = dL_ds * ds_dby\n",
    "    #          (1*10) - (1*10)     \n",
    "    dL_dby = (y_cap_ - y_).sum(axis = 1).reshape(-1,1)  #   (1*1)\n",
    "                        \n",
    "    #dL_dba = dL_da * da_dba\n",
    "    #       (2*10)       (2*1) (1*10)-(1*10) \n",
    "    dL_dba = (a_ * np.dot(Way, (y_cap_ - y_))).sum(axis = 1 ).reshape(-1,1)   # (2*10)\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dL_da0\" :dL_da0,\n",
    "                 \"dL_dWaa\": dL_dWaa, \n",
    "                 \"dL_dWax\": dL_dWax,\n",
    "                 \"dL_dWay\": dL_dWay,\n",
    "                 \"dL_dba\" : dL_dba,\n",
    "                 \"dL_dby\" : dL_dby\n",
    "                 }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(a0, activations, y_caps, y1_ohe, parameters,xt):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    da     -- Upstream gradients of all hidden states, of shape (batch_size, vocab_size, timestep)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                    dx  -- Gradient w.r.t. the input data, numpy-array of shape (batch_size, vocab_size, timestep)\n",
    "                    da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (hidden_unit, vocab_size)\n",
    "                    dWax-- Gradient w.r.t the input's weight matrix, numpy-array of shape (batch_size, hidden_unit)\n",
    "                    dWaa-- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (hidden_unit,hidden_unit)\n",
    "                    dba -- Gradient w.r.t the bias, of shape (hidden_unit, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, vocab_size, timestep =  y_caps.shape\n",
    "    hidden_unit = activations.shape[0]\n",
    "    \n",
    "    dWax = 0\n",
    "    dWay = 0\n",
    "    dWaa = 0\n",
    "    dba  = 0\n",
    "    da0  = 0\n",
    "    dby  = 0\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    \n",
    "    for t in range(timestep):\n",
    "        \n",
    "        gradient = rnn_cell_backward(t, y_caps, y1_ohe, activations, parameters,xt,a0)\n",
    "        \n",
    "        dL_da0      = gradient[\"dL_da0\"]\n",
    "        dL_dWaa     = gradient[\"dL_dWaa\"]\n",
    "        dL_dWax     = gradient[\"dL_dWax\"]\n",
    "        dL_dWay     = gradient[\"dL_dWay\"]   \n",
    "        dL_dba      = gradient[\"dL_dba\"]\n",
    "        dL_dby      = gradient[\"dL_dby\"]\n",
    "        \n",
    "        da0  += dL_da0\n",
    "        dWaa += dL_dWaa\n",
    "        dWax += dL_dWax\n",
    "        dWay += dL_dWay\n",
    "        dba  += dL_dba\n",
    "        dby  += dL_dby\n",
    "    \n",
    "    gradients = {\"da0\": da0, \"dWaa\": dWaa, \"dWax\": dWax, \"dWay\": dWay,\"dba\": dba,'dby':dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(y_caps,y1):\n",
    "    cost = []\n",
    "    for i in range(y_caps.shape[2]):\n",
    "        y_actual = (one_hot_vector_vocab[y1[i]]).reshape(1,-1)\n",
    "        y_pred   = y_caps[:,:,i]\n",
    "        cost.append(-np.sum(y_actual * np.log(y_pred)))\n",
    "    return np.mean(cost,axis=0)\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    one = np.zeros([1,10,4])\n",
    "    for i in range(len(y)):\n",
    "        one[:,:,i] = one_hot_vector_vocab[y1[i]].reshape(1,-1)\n",
    "    return one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost:  2.295090024207877\n",
      "Final cost:  2.2922161782315897\n",
      "Final cost:  2.2892558138074204\n",
      "Final cost:  2.2862029606399026\n",
      "Final cost:  2.2830518630043835\n",
      "Final cost:  2.2797970480459386\n",
      "Final cost:  2.2764334051634347\n",
      "Final cost:  2.2729562760142246\n",
      "Final cost:  2.269361553980236\n",
      "Final cost:  2.265645791096242\n",
      "Final cost:  2.261806309495847\n",
      "Final cost:  2.2578413134529094\n",
      "Final cost:  2.2537499971927333\n",
      "Final cost:  2.2495326429509666\n",
      "Final cost:  2.2451907034087877\n",
      "Final cost:  2.2407268627488346\n",
      "Final cost:  2.2361450712204274\n",
      "Final cost:  2.2314505492544834\n",
      "Final cost:  2.226649758713517\n",
      "Final cost:  2.221750340607662\n",
      "Final cost:  2.21676102032656\n",
      "Final cost:  2.211691482927092\n",
      "Final cost:  2.206552222156217\n",
      "Final cost:  2.2013543676629865\n",
      "Final cost:  2.1961094953477667\n",
      "Final cost:  2.190829426142775\n",
      "Final cost:  2.1855260188335137\n",
      "Final cost:  2.180210962861414\n",
      "Final cost:  2.1748955773452128\n",
      "Final cost:  2.169590622695794\n",
      "Final cost:  2.1643061310218\n",
      "Final cost:  2.1590512609092003\n",
      "Final cost:  2.1538341810667427\n",
      "Final cost:  2.1486619858226\n",
      "Final cost:  2.143540643687248\n",
      "Final cost:  2.1384749783681603\n",
      "Final cost:  2.1334686799423572\n",
      "Final cost:  2.128524342534442\n",
      "Final cost:  2.1236435239161935\n",
      "Final cost:  2.118826821971726\n",
      "Final cost:  2.1140739629275753\n",
      "Final cost:  2.10938389655265\n",
      "Final cost:  2.1047548940882246\n",
      "Final cost:  2.100184645369938\n",
      "Final cost:  2.0956703523602345\n",
      "Final cost:  2.0912088170489573\n",
      "Final cost:  2.086796522352224\n",
      "Final cost:  2.0824297052163776\n",
      "Final cost:  2.078104421603134\n",
      "Final cost:  2.0738166033947647\n",
      "Final cost:  2.069562107523344\n",
      "Final cost:  2.0653367578091673\n",
      "Final cost:  2.0611363801055154\n",
      "Final cost:  2.0569568314048867\n",
      "Final cost:  2.052794023579136\n",
      "Final cost:  2.0486439424142504\n",
      "Final cost:  2.0445026625691587\n",
      "Final cost:  2.0403663590445364\n",
      "Final cost:  2.036231315697602\n",
      "Final cost:  2.032093931286577\n",
      "Final cost:  2.0279507234766503\n",
      "Final cost:  2.023798331189897\n",
      "Final cost:  2.019633515635791\n",
      "Final cost:  2.0154531603174317\n",
      "Final cost:  2.0112542702715293\n",
      "Final cost:  2.0070339707676736\n",
      "Final cost:  2.002789505664122\n",
      "Final cost:  1.9985182355931\n",
      "Final cost:  1.994217636127964\n",
      "Final cost:  1.9898852960672098\n",
      "Final cost:  1.9855189159557405\n",
      "Final cost:  1.9811163069517352\n",
      "Final cost:  1.976675390137431\n",
      "Final cost:  1.9721941963638299\n",
      "Final cost:  1.9676708667124077\n",
      "Final cost:  1.9631036536510178\n",
      "Final cost:  1.9584909229560346\n",
      "Final cost:  1.9538311564680488\n",
      "Final cost:  1.949122955743824\n",
      "Final cost:  1.9443650466624092\n",
      "Final cost:  1.9395562850379682\n",
      "Final cost:  1.9346956632856358\n",
      "Final cost:  1.929782318179159\n",
      "Final cost:  1.924815539729769\n",
      "Final cost:  1.9197947812040446\n",
      "Final cost:  1.9147196702839273\n",
      "Final cost:  1.9095900213536268\n",
      "Final cost:  1.9044058488750712\n",
      "Final cost:  1.899167381784618\n",
      "Final cost:  1.8938750788075964\n",
      "Final cost:  1.8885296445423327\n",
      "Final cost:  1.8831320461096568\n",
      "Final cost:  1.877683530095478\n",
      "Final cost:  1.872185639430447\n",
      "Final cost:  1.86664022974966\n",
      "Final cost:  1.8610494846545471\n",
      "Final cost:  1.8554159291569285\n",
      "Final cost:  1.8497424404211027\n",
      "Final cost:  1.8440322547352848\n",
      "Final cost:  1.8382889694432532\n"
     ]
    }
   ],
   "source": [
    "x1 = x1  # change to x2, x3 for more results\n",
    "y1 = y1  # change to y2, y3 for more results\n",
    "np.random.seed(1)\n",
    "x1  = x1.reshape(1,10,4)     # (1*10*4) ==> batch_size    * vocab_size * timestep \n",
    "Wax = np.random.randn(1,2)   # (1*2)    ==> batch_size    * hidden_neuron\n",
    "\n",
    "a0  = np.random.randn(2,10)  # (2*10)   ==> hidden_neuron * vocab_size\n",
    "Waa = np.random.randn(2,2)   # (2*2)    ==> hidden_neuron * hidden_neuron\n",
    "ba  = np.random.randn(2,1)   # (2*1)    ==> hidden nueron * 1\n",
    "\n",
    "Way = np.random.randn(2,1)   # (2*1)    ==> hidden_neuron * output_neuron(or batch_size)\n",
    "by  = np.random.randn(1,1)   # (1*1)    ==> output_neuron * 1\n",
    "\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "\n",
    "y = one_hot_encode(y1)\n",
    "\n",
    "#######################\n",
    "# stochastic gradient #\n",
    "#######################\n",
    "alpha = 0.01\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Way\": Way, \"ba\": ba, \"by\": by}\n",
    "for i in range(100):\n",
    "    a0, activations, y_caps = rnn_forward(x1, a0, parameters)\n",
    "    gradients               = rnn_backward(a0, activations, y_caps, y, parameters,xt)\n",
    "    print(\"Final cost: \",crossEntropy(y_caps,y1))\n",
    "    parameters['Waa'] -= (alpha * gradients['dWaa'])\n",
    "    parameters['Wax'] -= (alpha * gradients['dWax'])\n",
    "    parameters['Way'] -= (alpha * gradients['dWay'])\n",
    "    parameters['ba']  -= (alpha * gradients['dba'])\n",
    "    parameters['by']  -= (alpha * gradients['dby'])\n",
    "    parameters['by']  -= (alpha * gradients['dby'])\n",
    "    a0                -= (alpha * gradients['da0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(y_caps[:,:,0]))\n",
    "print(np.argmax(y_caps[:,:,1]))\n",
    "print(np.argmax(y_caps[:,:,2]))\n",
    "print(np.argmax(y_caps[:,:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual_input\tActual_expected\t\t\tPredicted\n",
      "mango\t\t\tis\t\t\tcolor\n",
      "is\t\t\tyellow\t\t\tcolor\n",
      "yellow\t\t\tcolor\t\t\tcolor\n",
      "color\t\t\t<end>\t\t\tbanana\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual_input\\tActual_expected\\t\\t\\tPredicted\")\n",
    "for i in range(4):\n",
    "    print(\"{}\\t\\t\\t{}\\t\\t\\t{}\".format(idx2char[np.argmax(x1[:,:,i])],idx2char[y1[i]],idx2char[np.argmax(y_caps[:,:,i])]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) / temperature # shift each row of y_linear by its max\n",
    "    exp = nd.exp(lin)\n",
    "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
