{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is KNN??\n",
    "\n",
    "#### KNN (K — Nearest Neighbors) is one of many (supervised learning) algorithms used in data mining and machine learning, it’s a classifier algorithm where the learning is based “how similar” is a data (a vector) from other ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n",
    "\n",
    "In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification.\n",
    "\n",
    "Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\n",
    "\n",
    "The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n",
    "\n",
    "A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n",
    "\n",
    "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n",
    "\n",
    "A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric. Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.\n",
    "\n",
    "A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number. \n",
    "\n",
    "One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN steps:\n",
    "\n",
    "The KNN’s steps are:\n",
    "1. — Receive an unclassified data;\n",
    "2. — Measure the distance (Euclidian, Manhattan, Minkowski or Weighted) from the new data to all others data that is already classified;\n",
    "3. — Gets the K(K is a parameter that you difine) smaller distances;\n",
    "4. — Check the list of classes had the shortest distance and count the amount of each class that appears;\n",
    "5. — Takes as correct class the class that appeared the most times;\n",
    "6. —Classifies the new data with the class that you took in step 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidian formula:\n",
    "$$\n",
    "\\begin{aligned} d(\\mathbf{p}, \\mathbf{q})=d(\\mathbf{q}, \\mathbf{p}) &=\\sqrt{\\left(q_{1}-p_{1}\\right)^{2}+\\left(q_{2}-p_{2}\\right)^{2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(q_{i}-p_{i}\\right)^{2}} \\end{aligned}\n",
    "$$\n",
    "\n",
    "where :\n",
    "\n",
    "q1,q2,q3,..,qn are the feature point of 1 training example\n",
    "\n",
    "p1,p2,p3,..,pn are the feature point of 1 testing example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we choose the factor K?\n",
    "\n",
    "The training error rate and the validation error rate are two parameters we need to access on different K-value. \n",
    "\n",
    "Following is the curve for the training error rate with varying value of K :\n",
    "\n",
    "<img src=\"training-error.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself.Hence the prediction is always accurate with K=1. If validation error curve would have been similar, our choice of K would have been 1. Following is the validation error curve with varying value of K:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"training-error_11.png\">\n",
    "\n",
    "This makes the story more clear. At K=1, we were overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PseudoCode for kNN\n",
    "\n",
    "We can implement a KNN model by following the below steps:\n",
    "\n",
    "1. Load the data\n",
    "2. Initialise the value of k\n",
    "3. For getting the predicted class, iterate from 1 to total number of training data points\n",
    "    1. Calculate the distance between test data and each row of training data. Here we will use Euclidean distance as our distance metric since it’s the most popular method. The other metrics that can be used are Chebyshev, cosine, etc.\n",
    "    2. Sort the calculated distances in ascending order based on distance values\n",
    "    3. Get top k rows from the sorted array\n",
    "    4. Get the most frequent class of these rows\n",
    "    5. Return the predicted class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
