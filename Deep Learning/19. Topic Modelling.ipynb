{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler)\n",
    "print(\"Tensorflow : \",tf.__version__)\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = \".../multiclass_classification_data.csv\"\n",
    "data = pd.read_csv(path_df)\n",
    "data.loc[:,'Short_Text']  = data['Short_Text'].str.lower().str.strip()\n",
    "data.loc[:,'ML_Category'] = data['ML_Category'].str.lower().str.strip()\n",
    "data.loc[:,'data_type']   = data['data_type'].str.lower().str.strip()\n",
    "data = data.dropna().drop_duplicates()\n",
    "data = data.rename({\"Short_Text\":'text', 'ML_Category':'y'}, axis=1)\n",
    "data = shuffle(data)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.data_type.isin(['train', 'test', 'val'])].reset_index(drop=True)\n",
    "x_train, y_train = train[['text', 'y']],  train[['y']]\n",
    "x_train = x_train.rename({\"y\":'y_true'}, axis=1)\n",
    "\n",
    "test = data[data.data_type.isin(['val'])].reset_index(drop=True)\n",
    "x_test, y_test = test[['text', 'y']], test[['y']]\n",
    "x_test = x_test.rename({\"y\":'y_true'}, axis=1)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissKMeans:\n",
    "    def __init__(self, n_clusters=135, n_init=10, max_iter=100, n_redo=1, verbose=False, seed=None):\n",
    "      self.n_clusters = n_clusters\n",
    "      self.n_init = n_init\n",
    "      self.max_iter = max_iter\n",
    "      self.n_redo = n_redo\n",
    "      self.verbose = verbose\n",
    "      self.seed = seed\n",
    "      self.kmeans = None\n",
    "      self.cluster_centers_ = None\n",
    "      self.inertia_ = None\n",
    "\n",
    "    def sampling(self,which=None):\n",
    "        pass\n",
    "      \n",
    "    def fit(self, X):\n",
    "      '''\n",
    "      nredo         : run the clustering this number of times, and keep the best centroids (selected according to clustering objective)\n",
    "      verbose       : make clustering more verbose\n",
    "      spherical     : perform spherical k-means -- the centroids are L2 normalized after each iteration\n",
    "      int_centroids : round centroids coordinates to integer\n",
    "      update_index  : re-train index after each iteration?\n",
    "      min_points_per_centroid / max_points_per_centroid: below, you get a warning, above the training set is subsampled\n",
    "      seed          : seed for the random number generator\n",
    "      '''\n",
    "      self.kmeans = faiss.Kmeans(d=X.shape[1],\n",
    "                                 k=self.n_clusters,\n",
    "                                 niter=self.max_iter,\n",
    "                                 nredo=self.n_init,\n",
    "                                 verbose=True, \n",
    "                                 gpu=True)\n",
    "      self.kmeans.train(X.astype(np.float32))\n",
    "      self.cluster_centers_ = self.kmeans.centroids\n",
    "      self.inertia_ = self.kmeans.obj[-1]\n",
    "\n",
    "    def get_best_cluster(self, X):\n",
    "      '''\n",
    "      .search(1): 1 use to fetch 1 nearest cluster\n",
    "      '''\n",
    "      return self.kmeans.index.search(X.astype(np.float32), 1)\n",
    "      \n",
    "    def iteration_stats(self):\n",
    "      return self.kmeans.iteration_stats\n",
    "    \n",
    "    def inertia(self):\n",
    "      return self.kmeans.obj\n",
    "    \n",
    "    def get_pred_labels(self, x_data, y_data):\n",
    "      '''\n",
    "      model  : FaissKMeans object\n",
    "      x_data : numpy [batch,384] array\n",
    "      y_data : pandas df have single column name y\n",
    "      '''\n",
    "      D, I = self.get_best_cluster(x_data)\n",
    "      y_pred_cluster_mapping = {i:y_data.loc[np.argwhere(I.flatten()==i).flatten().flatten(),'y'].mode().values[0] for i in np.unique(I)}\n",
    "      y_pred = np.array(list(map(lambda x: y_pred_cluster_mapping[x], I.flatten())))\n",
    "      return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalers(train, test, scale=None):\n",
    "  '''\n",
    "  Train: Embedded train set\n",
    "  Test : Embedded Test set\n",
    "  Scale: None, minmax, standard\n",
    "  '''\n",
    "  if scale == 'minmax':\n",
    "    scaler = MinMaxScaler()\n",
    "    X_TRAIN_S = scaler.fit_transform(train)\n",
    "    X_TEST_S  = scaler.transform(test)\n",
    "  elif scale == 'standard':\n",
    "    scaler = StandardScaler()\n",
    "    X_TRAIN_S = scaler.fit_transform(train)\n",
    "    X_TEST_S  = scaler.transform(test)\n",
    "  else:\n",
    "    scaler = None\n",
    "    X_TRAIN_S = train\n",
    "    X_TEST_S  = test\n",
    "  return X_TRAIN_S, X_TEST_S, scaler\n",
    "\n",
    "\n",
    "def which_embedding_model(name):\n",
    "  ''' \n",
    "  name =\n",
    "  tensorflow_use : Tensorflow Universal Sentense Embedding 512 dim.\n",
    "  sbert          : 384 dim\n",
    "  sbert_all      : 384 dim\n",
    "  mpnet_all      : 768 dim\n",
    "  '''  \n",
    "  if name=='tensorflow_use': \n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    sentense_model = hub.load(module_url)\n",
    "    return lambda x: tf.squeeze(sentense_model([x]), axis=0)\n",
    "  \n",
    "  if name=='sbert': \n",
    "    sentense_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    return lambda x: sentense_model.encode(x)\n",
    "  \n",
    "  if name=='sbert_all': \n",
    "    sentense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return lambda x: sentense_model.encode(x)\n",
    "  \n",
    "  if name=='mpnet_all': \n",
    "    sentense_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    return lambda x: sentense_model.encode(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings = which_embedding_model('mpnet_all')\n",
    "x_train['emb'] = x_train['text'].apply(get_embeddings)\n",
    "x_test['emb']  = x_test['text'].apply(get_embeddings)\n",
    "\n",
    "X_TRAIN = np.array(x_train['emb'].tolist(),dtype=np.float16) # [instance, 512]\n",
    "X_TEST = np.array(x_test['emb'].tolist(),dtype=np.float16) # [instance, 512]\n",
    "print(X_TRAIN.shape, X_TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# For single cluster run #\n",
    "##########################\n",
    "all_models = []\n",
    "k_range = [400, 510]\n",
    "scales = None\n",
    "X_TRAIN_S, X_TEST_S, scaler = scalers(X_TRAIN, X_TEST, scales)\n",
    "for n_clusters in k_range:\n",
    "  model = FaissKMeans(n_clusters=n_clusters, n_init=10, max_iter=100)\n",
    "  model.fit(X_TRAIN_S)\n",
    "  silhouette_scores = silhouette_score(X_TRAIN_S, model.get_best_cluster(X_TRAIN_S)[1].ravel())\n",
    "  inertia          = model.kmeans.obj[-1]\n",
    "  y_pred = model.get_pred_labels(X_TEST_S, y_test)\n",
    "  acc = accuracy_score(x_test.y_true, y_pred)\n",
    "  precision, recall, fscore, _ = precision_recall_fscore_support(x_test.y_true, y_pred, average='macro',labels=np.unique(x_test.y_true))\n",
    "  all_models.append({'scale':str(scales), 'k':n_clusters, 'sil':silhouette_scores, 'iner':inertia,  \n",
    "                     'prec':precision, 'recall':recall, \"fscore\":fscore, 'acc':acc})\n",
    "  print(\"\\tDone: \",n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = pd.DataFrame(all_models)\n",
    "print(model_output[model_output['scale']=='None'].sort_values(['acc', 'fscore'], ascending=False).head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
