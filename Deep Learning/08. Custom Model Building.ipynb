{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Modellig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cls():\n",
    "    tf.random.set_seed(42)\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent variables :  (16718, 8) (1858, 8) (2064, 8)\n",
      "Target variables      :  (16718,) (1858,) (2064,)\n"
     ]
    }
   ],
   "source": [
    "house = fetch_california_housing()\n",
    "x_train, x_test, y_train, y_test   = train_test_split(house.data, house.target, test_size=0.1, random_state=42)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "scaler, scaler2 = StandardScaler(), StandardScaler()\n",
    "x_train_s = scaler.fit_transform(x_train)\n",
    "x_test_s  = scaler.transform(x_test)\n",
    "x_valid_s = scaler.transform(x_valid)\n",
    "\n",
    "print(\"Independent variables : \", x_train_s.shape, x_valid_s.shape, x_test_s.shape)\n",
    "print(\"Target variables      : \", y_train.shape, y_valid.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.4583 - mae: 0.7910 - val_loss: 0.2270 - val_mae: 0.5280\n",
      "Epoch 2/4\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1969 - mae: 0.4809 - val_loss: 0.2079 - val_mae: 0.4939\n",
      "Epoch 3/4\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1833 - mae: 0.4600 - val_loss: 0.1921 - val_mae: 0.4709\n",
      "Epoch 4/4\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1747 - mae: 0.4477 - val_loss: 0.1878 - val_mae: 0.4646\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1816 - mae: 0.4583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18163689970970154, 0.4582922160625458]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls()\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\", input_shape=[x_train_s.shape[1]]),\n",
    "                            tf.keras.layers.Dense(10, activation=\"elu\"),\n",
    "                            tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer='adam', metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=4, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Custom Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create huber loss function\n",
    "def huber_fun(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2 # Normalize MSE\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "z_center = np.linspace(-1, 1, 200)\n",
    "plt.plot(z, huber_fun(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z ** 2 / 2, \"r:\", linewidth=1)\n",
    "plt.plot(z_center, z_center ** 2 / 2, \"r\", linewidth=2)\n",
    "plt.plot([-1, -1], [0, huber_fun(0., -1.)], \"k--\")\n",
    "plt.plot([1, 1], [0, huber_fun(0., 1.)], \"k--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.text(2.1, 3.5, r\"$\\frac{1}{2}z^2$\", color=\"r\", fontsize=15)\n",
    "plt.text(3.0, 2.2, r\"$|z| - \\frac{1}{2}$\", color=\"b\", fontsize=15)\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1370 - mae: 0.3849 - val_loss: 0.1480 - val_mae: 0.4024\n",
      "Epoch 2/4\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1365 - mae: 0.3843 - val_loss: 0.1463 - val_mae: 0.4000\n",
      "Epoch 3/4\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1362 - mae: 0.3829 - val_loss: 0.1466 - val_mae: 0.4036\n",
      "Epoch 4/4\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1356 - mae: 0.3822 - val_loss: 0.1471 - val_mae: 0.3987\n",
      "65/65 [==============================] - 0s 828us/step - loss: 0.1445 - mae: 0.3964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1445251703262329, 0.39644020795822144]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls()\n",
    "model.compile(loss=huber_fun, optimizer='adam', metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=4, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saving/Loading with custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[195.63742]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to pass all custom objects\n",
    "temp = tf.keras.models.load_model(\"model/model.h5\", custom_objects={\"huber_fun\":huber_fun})\n",
    "temp.predict(x_test[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1350 - mae: 0.3812 - val_loss: 0.1463 - val_mae: 0.3990\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1347 - mae: 0.3809 - val_loss: 0.1459 - val_mae: 0.3984\n",
      "65/65 [==============================] - 0s 794us/step - loss: 0.1420 - mae: 0.3940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14201784133911133, 0.39404675364494324]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fun(y_true, y_pred):\n",
    "        error = y_true-y_pred\n",
    "        is_error_small = error < threshold\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_error_small, squared_loss, linear_loss)\n",
    "    return huber_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1454 - mae: 0.3758 - val_loss: 0.1629 - val_mae: 0.3934\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1438 - mae: 0.3751 - val_loss: 0.1604 - val_mae: 0.3928\n",
      "65/65 [==============================] - 0s 946us/step - loss: 0.1552 - mae: 0.3916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1551503688097, 0.3916175663471222]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls()\n",
    "model.compile(loss=create_huber(2.), optimizer='adam', metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/model_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to specify threshold value manually\n",
    "model2 = tf.keras.models.load_model(\"model/model_2.h5\", custom_objects={\"huber_fun\":create_huber(2.)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1438 - mae: 0.3744 - val_loss: 0.1618 - val_mae: 0.3921\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1436 - mae: 0.3744 - val_loss: 0.1606 - val_mae: 0.3923\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1552 - mae: 0.3916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1551503688097, 0.3916175663471222]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model2.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can solve this problem by implementing tf.keras.losses.Loss class and its get_config() function\n",
    "\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1., **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred): # This function called during training. You can do anything in this function\n",
    "        error = y_true - y_pred\n",
    "        is_error_small = error < self.threshold\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_error_small, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\":self.threshold}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1449 - mae: 0.3752 - val_loss: 0.1624 - val_mae: 0.3926\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.1434 - mae: 0.3744 - val_loss: 0.1602 - val_mae: 0.3921\n",
      "65/65 [==============================] - 0s 960us/step - loss: 0.1548 - mae: 0.3910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15481024980545044, 0.39101728796958923]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls()\n",
    "model.compile(loss=HuberLoss(2.), optimizer='adam', metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/model_custom_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1434 - mae: 0.3738 - val_loss: 0.1611 - val_mae: 0.3912\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.1432 - mae: 0.3738 - val_loss: 0.1602 - val_mae: 0.3916\n",
      "65/65 [==============================] - 0s 960us/step - loss: 0.1548 - mae: 0.3910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15481024980545044, 0.39101728796958923]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model running fine without pass explicit threshold value\n",
    "model3 = tf.keras.models.load_model('model/model_custom_class.h5', custom_objects={\"HuberLoss\":HuberLoss})\n",
    "history = model3.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduction': 'auto', 'name': None, 'threshold': 2.0}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Activation function, Initializers, Regularizers, Constraints, and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softplus activation function\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorat(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2./(shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# 0.01 regularize threshold\n",
    "def my_l1_regularize(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights)) \n",
    "\n",
    "# return value is just tf.nn.relu(weights)\n",
    "def my_positive_weights(weights): \n",
    "    return tf.where(weights<0.,tf.zeros_like(weights), weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =tf.keras.layers.Dense(1,activation=my_softplus,kernel_initializer=my_glorat,kernel_regularizer=my_l1_regularize,kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 1.6073 - mae: 1.3172 - val_loss: 0.5062 - val_mae: 0.5759\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.4243 - mae: 0.5259 - val_loss: 0.4102 - val_mae: 0.5260\n",
      "65/65 [==============================] - 0s 843us/step - loss: 0.4028 - mae: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40281733870506287, 0.5240435600280762]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets build one model again\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(30, kernel_initializer=my_glorat, input_shape=(x_train_s.shape[1:])),\n",
    "                                    tf.keras.layers.Dense(10, activation=my_softplus,\n",
    "                                                            kernel_initializer=my_glorat, \n",
    "                                                            kernel_regularizer=my_l1_regularize, \n",
    "                                                            kernel_constraint=my_positive_weights),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"adam\", metrics=['mae'])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.3616 - mae: 0.4979 - val_loss: 0.3649 - val_mae: 0.5126\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.3255 - mae: 0.4873 - val_loss: 0.3331 - val_mae: 0.5007\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3230 - mae: 0.496 - 0s 938us/step - loss: 0.3233 - mae: 0.4975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3233291208744049, 0.4975474774837494]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"model/model_custom_kernels.h5\")\n",
    "co = {\"my_glorat\":my_glorat,\"my_softplus\":my_softplus,\"my_l1_regularize\":my_l1_regularize,\"my_positive_weights\":my_positive_weights,\"HuberLoss\":HuberLoss}\n",
    "model4 = tf.keras.models.load_model(\"model/model_custom_kernels.h5\", custom_objects=co)\n",
    "history = model4.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model4.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to build with custom hyperparameters\n",
    "################################################################################################################################################\n",
    "# We must implement __call__() method for regularizers, initializers, and constraints parent class do not define get_config() function as well #\n",
    "# and implement call() for losses, layers(including activation functions), and models                                                          #\n",
    "################################################################################################################################################\n",
    "class MyL1Regular(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, threshold, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        # Not calling parent class's constructor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.threshold*weights))\n",
    "    def get_config(self):\n",
    "        return {\"threshold\":self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 1.7757 - mae: 1.3462 - val_loss: 0.5867 - val_mae: 0.5782\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.4763 - mae: 0.5345 - val_loss: 0.4368 - val_mae: 0.5324\n",
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.3721 - mae: 0.5075 - val_loss: 0.3624 - val_mae: 0.5191\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.3210 - mae: 0.4941 - val_loss: 0.3313 - val_mae: 0.5061\n",
      "65/65 [==============================] - 0s 976us/step - loss: 0.3236 - mae: 0.5031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3235660493373871, 0.5030665397644043]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets build one model again\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(30, kernel_initializer=my_glorat, input_shape=(x_train_s.shape[1:])),\n",
    "                                    tf.keras.layers.Dense(10, activation=my_softplus,\n",
    "                                                            kernel_initializer=my_glorat, \n",
    "                                                            kernel_regularizer=MyL1Regular(0.02), \n",
    "                                                            kernel_constraint=my_positive_weights),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"adam\", metrics=['mae'])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.save(\"model/model_custom_class_kernels.h5\")\n",
    "co = {\"my_glorat\":my_glorat,\"my_softplus\":my_softplus,\"MyL1Regular\":MyL1Regular,\"my_positive_weights\":my_positive_weights,\"HuberLoss\":HuberLoss}\n",
    "model5 = tf.keras.models.load_model(\"model/model_custom_class_kernels.h5\", custom_objects=co)\n",
    "# Retraining to check whether it is working or not\n",
    "history = model5.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model5.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Skip this for instance: come back here after 9. custom model #\n",
    "# To better understanding TO build custom optimizers           #\n",
    "################################################################\n",
    "class MyMomentumOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._set_hyper(\"decay\", self._initial_decay) # \n",
    "        self._set_hyper(\"momentum\", momentum)\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"momentum\")\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        momentum_var = self.get_slot(var, \"momentum\")\n",
    "        momentum_hyper = self._get_hyper(\"momentum\", var_dtype)\n",
    "        momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad)\n",
    "        var.assign_add(momentum_var * lr_t)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "523/523 [==============================] - 1s 607us/step - loss: 3.1587\n",
      "Epoch 2/5\n",
      "523/523 [==============================] - 0s 661us/step - loss: 1.0364\n",
      "Epoch 3/5\n",
      "523/523 [==============================] - 0s 556us/step - loss: 0.7367\n",
      "Epoch 4/5\n",
      "523/523 [==============================] - 0s 572us/step - loss: 0.6734\n",
      "Epoch 5/5\n",
      "523/523 [==============================] - 0s 596us/step - loss: 0.6465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21069f1d520>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls()\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[8])])\n",
    "model.compile(loss=\"mse\", optimizer=MyMomentumOptimizer())\n",
    "model.fit(x_train_s, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Custom Metrics\n",
    " Defining the custom metrics exactly the same as defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.3494 - huber_fun: 0.2392 - val_loss: 0.3414 - val_huber_fun: 0.2542\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.3054 - huber_fun: 0.2263 - val_loss: 0.3183 - val_huber_fun: 0.2449\n",
      "65/65 [==============================] - 0s 957us/step - loss: 0.3107 - huber_fun: 0.2374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31067290902137756, 0.237367644906044]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.), optimizer='adam', metrics=[create_huber(2.)])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you use the same function as the loss and a metric, you may be surprised to see slightly different results. This is in part because the operations are not computed exactly in the same order, so there might be tiny floating point errors. More importantly, if you use sample weights or class weights, then the equations are a bit different:\n",
    "\n",
    "- the fit() method keeps track of the mean of all batch losses seen so far since the start of the epoch. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so ***the batch loss is not the weighted mean of the losses***).\n",
    "- the metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is ***the weighted mean of all the instance losses. Not the same thing***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to keep track the mean loss or mean accuracy in epoch during batch run we implement something called **Streaming metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls()\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # Handle base arguments like (eg: dtypes)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn  = create_huber(threshold)\n",
    "        self.total     = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count     = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total/ self.count\n",
    "\n",
    "    def get_config(self):\n",
    "        return {**super().get_config(), \"threshold\":self.threshold} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=32.0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = HuberMetric(2.)\n",
    "\n",
    "# total = 2 * |10 - 2| - 2/2 = 14\n",
    "# count = 1\n",
    "# result = 14 / 1 = 14\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=13.84375>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total = total + (|1 - 0| / 2) + (2 * |9.25 - 5| - 2 / 2) = 14 + 7 = 21\n",
    "# count = count + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7\n",
    "m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=13.84375>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=41.53125>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reset_states()\n",
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 3s 2ms/step - loss: 0.2860 - huber_metric_1: 0.2214 - val_loss: 0.3025 - val_huber_metric_1: 0.2439\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.2740 - huber_metric_1: 0.2186 - val_loss: 0.2923 - val_huber_metric_1: 0.2403\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2839 - huber_metric_1: 0.2319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28389590978622437, 0.2319127470254898]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check metrics custom class working or not\n",
    "model.compile(loss=HuberLoss(2.), optimizer='adam', metrics=[HuberMetric(2.)])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.2682 - huber_metric_1: 0.2173 - val_loss: 0.2899 - val_huber_metric_1: 0.2419\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.2625 - huber_metric_1: 0.2169 - val_loss: 0.2826 - val_huber_metric_1: 0.2394\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3236 - mae: 0.5031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3235660493373871, 0.5030665397644043]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"model/model_custom_metric_kernels.h5\")\n",
    "co = {\"my_glorat\":my_glorat,\n",
    "        \"my_softplus\":my_softplus,\n",
    "        \"MyL1Regular\":MyL1Regular,\n",
    "        \"my_positive_weights\":my_positive_weights,\n",
    "        \"HuberLoss\":HuberLoss,\n",
    "        \"HuberMetric\":HuberMetric}\n",
    "model6 = tf.keras.models.load_model(\"model/model_custom_metric_kernels.h5\", custom_objects=co)\n",
    "# Retraining to check whether it is working or not\n",
    "history = model6.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "model5.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.metrics` contains the model's loss followed by the model's metric(s), so the `HuberMetric` is `model.metrics[-1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.HuberMetric at 0x21064ff7460>, 2.0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.metrics[-1], model6.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create the above class like this:\n",
    "cls()\n",
    "class HuberMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 1.0166 - HuberMetric: 1.5222 - val_loss: 0.4716 - val_HuberMetric: 0.3186\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.2426 - HuberMetric: 0.2736 - val_loss: 0.3575 - val_HuberMetric: 0.2863\n"
     ]
    }
   ],
   "source": [
    "# Lets build one model again\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(30, kernel_initializer=my_glorat, input_shape=(x_train_s.shape[1:])),\n",
    "                                    tf.keras.layers.Dense(10, activation=my_softplus,\n",
    "                                                            kernel_initializer=my_glorat, \n",
    "                                                            kernel_regularizer=MyL1Regular(0.02), \n",
    "                                                            kernel_constraint=my_positive_weights),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"adam\", metrics=[HuberMetric(2.0)])\n",
    "sample_weights = tf.random.uniform([len(y_train)], 0, 1)\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid), sample_weight=sample_weights)\n",
    "\n",
    "#model.save(\"model/model_custom_class_metrics.h5\")\n",
    "#co = {\"my_glorat\":my_glorat,\"my_softplus\":my_softplus,\"MyL1Regular\":MyL1Regular,\"my_positive_weights\":my_positive_weights,\"HuberLoss\":HuberLoss}\n",
    "# model5 = tf.keras.models.load_model(\"model/model_custom_class_metrics.h5\", custom_objects=co)\n",
    "# # Retraining to check whether it is working or not\n",
    "# history = model5.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "# model5.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0165752172470093, <tf.Tensor: shape=(), dtype=float32, numpy=0.762183>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(history.history[\"loss\"][0], history.history[\"HuberMetric\"][0] * tf.reduce_mean(sample_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Layer\n",
    "We may occasionally want to build an architecture that contains block or layer or repeated layers like ABCD ABCD ABCD in that scenario we can create the custom layer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=20.085537>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use layers as a function also\n",
    "exp_layer(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.8528 - mae: 0.6322 - val_loss: 0.6336 - val_mae: 0.5701\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 0s 879us/step - loss: 0.5675 - mae: 0.5442 - val_loss: 0.5856 - val_mae: 0.5510\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(30,input_shape=(x_train_s.shape[1:])),\n",
    "                                    tf.keras.layers.Dense(1),\n",
    "                                    exp_layer])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple layer\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name=\"kernel\", shape=[batch_input_shape[-1], self.units], initializer=\"he_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)  # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units, \"activation\": tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 1.2230 - mae: 0.7961 - val_loss: 0.6848 - val_mae: 0.5939\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 0s 787us/step - loss: 0.5376 - mae: 0.5344 - val_loss: 0.5389 - val_mae: 0.5321\n",
      "INFO:tensorflow:Assets written to: model/model_custom_layer\\assets\n",
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.4473 - mae: 0.4815 - val_loss: 0.4694 - val_mae: 0.4939\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.5952 - mae: 0.4707 - val_loss: 0.4744 - val_mae: 0.4894\n",
      "65/65 [==============================] - 0s 608us/step - loss: 0.4453 - mae: 0.4701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4452732503414154, 0.4700668156147003]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([MyDense(30,activation=\"relu\", input_shape=(x_train_s.shape[1:])),\n",
    "                                    MyDense(1),\n",
    "                                    exp_layer])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "\n",
    "# We cannot write model in h5 format while include custom layers\n",
    "save_path = \"model/model_custom_layer\" \n",
    "model.save(save_path)\n",
    "\n",
    "co = {\"MYDense\":MyDense,\"exp_layer\":exp_layer}\n",
    "new_model = tf.keras.models.load_model(save_path, custom_objects=co)\n",
    "history = new_model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "new_model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer with multi output\n",
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape)  # extra code\n",
    "        return X1 + X2, X1 * X2, X1 / X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape1, batch_input_shape1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (None, 2)  X2.shape:  (None, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer')>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use custom layer as functional API\n",
    "cls()\n",
    "l1 = tf.keras.layers.Input(shape=[2])\n",
    "l2 = tf.keras.layers.Input(shape=[2])\n",
    "MyMultiLayer()((l1, l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (2, 2)  X2.shape:  (2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 9., 18.],\n",
       "        [ 6., 10.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[18., 72.],\n",
       "        [ 8., 21.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.5      , 0.5      ],\n",
       "        [0.5      , 2.3333333]], dtype=float32)>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can pass actual data as well\n",
    "X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]]) \n",
    "MyMultiLayer()((X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If our layers have different behavior during training and testing like dropout, BatchNormalization in that case we add training argument in call()\n",
    "\n",
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 2.7081 - mae: 1.1351 - val_loss: 1.5782 - val_mae: 0.9394\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 0s 904us/step - loss: 1.6358 - mae: 0.9626 - val_loss: 1.2550 - val_mae: 0.8148\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([MyGaussianNoise(1., input_shape=(x_train_s.shape[1:])),\n",
    "                                    MyDense(30,activation=\"relu\"),\n",
    "                                    MyDense(1),\n",
    "                                    exp_layer])\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Custom Models\n",
    "\n",
    "Its also straightforward create the subclass of `tf.keras.Model` create layers, and variables in the constructor, and implement call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets build a model that contains a skip connection block\n",
    "# It is a block\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is model which use residualblock\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 3.1743 - mae: 1.0238 - val_loss: 0.8021 - val_mae: 0.6305\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.6809 - mae: 0.5977 - val_loss: 0.8468 - val_mae: 0.6367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_2_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_3_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_custom_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/model_custom_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 1.0686 - mae: 0.6106 - val_loss: 0.5626 - val_mae: 0.5358\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 1ms/step - loss: 0.5641 - mae: 0.5189 - val_loss: 1.0523 - val_mae: 0.5250\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5094 - mae: 0.5095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5093919634819031, 0.5095109343528748]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls()\n",
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "\n",
    "save_path = \"model/model_custom_model\" \n",
    "model.save(save_path)\n",
    "\n",
    "new_model = tf.keras.models.load_model(save_path)\n",
    "history = new_model.fit(x_train_s, y_train, epochs=2, validation_data=(x_valid_s, y_valid))\n",
    "new_model.evaluate(x_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"residual_regressor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  270       \n",
      "_________________________________________________________________\n",
      "residual_block (ResidualBloc multiple                  1860      \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl multiple                  1860      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  31        \n",
      "=================================================================\n",
      "Total params: 4,021\n",
      "Trainable params: 4,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We can also build model like sequentially\n",
    "cls()\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model_ = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",kernel_initializer=\"he_normal\", input_shape=[x_train.shape[1]]),\n",
    "    block1, \n",
    "    block1, \n",
    "    block1, \n",
    "    block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now as of build model custom MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Losses & Metrics Based on Model Internals\n",
    "Till now we define loss and metrics function based on labels and the predictions (and optionally sample weights). If we want to define losses based on other parts of our model, such as the weights or activations of its hidden layers. this may be useful for regularization purposes or to monitor some internal aspect of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\") for _ in range(5)] # 5 hidden layers\n",
    "        self.out = tf.keras.layers.Dense(output_dim) # 1 output layer\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "        self.built = True  # WORKAROUND for super().build(batch_input_shape) <--- this throughing error right now\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.7760 - reconstruction_error: 0.8690\n",
      "Epoch 2/2\n",
      "523/523 [==============================] - 1s 2ms/step - loss: 0.4140 - reconstruction_error: 0.6491\n"
     ]
    }
   ],
   "source": [
    "cls()\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(x_train_s, y_train, epochs=2)\n",
    "y_pred = model.predict(x_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Computing Gradients using AutoDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.00000301118689"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "w1, w2 = 5, 5\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=40.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(5.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "# You can call tape.gradient() only once\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)  # returns tensor 40.0\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)  # raises a RuntimeError!\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use persistent=True if you want to use tape.gradient multiple times\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)  # returns tensor 40.0\n",
    "dz_dw2 = tape.gradient(z, w2)  # returns tensor 10.0, works fine now!\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=40.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw1, dz_dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor will not track the gradient of constant it only track the gradient of variables by default\n",
    "c1, c2 = tf.constant(5.), tf.constant(5.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=40.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to call tape.watch() to track the gradient of constants\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=40.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=80.0>]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx    = lambda x: 2*x**2 # This is f(x) = 2x^2 df(x)/dx = 4x, at x=10, df(10)/dx = 4*10 = 40 \n",
    "a1,a2 = tf.Variable(10.), tf.Variable(20.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = fx(a1)\n",
    "    z2 = fx(a2)\n",
    "tape.gradient([z1, z2], [a1, a2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=148.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if given a vector, tape.gradient() will compute the gradient of the vector's sum.\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=148.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows that we get the same result as the previous cell\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "    z = z1 + z2 + z3\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows how to compute the jacobians and the hessians\n",
    "fx    = lambda x: 2*x**2 # This is f(x) = 2x^2 df(x)/dx = 4x, at x=10, df(10)/dx = 4*10 = 40 \n",
    "a1,a2 = tf.Variable(10.), tf.Variable(20.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z1 = fx(a1)\n",
    "        z2 = fx(a2)\n",
    "    jacobians = jacobian_tape.gradient([z1, z2], [a1, a2])\n",
    "hessians = [hessian_tape.gradient(jacobian, [a1, a2]) for jacobian in jacobians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=40.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=80.0>]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=4.0>, None],\n",
       " [None, <tf.Tensor: shape=(), dtype=float32, numpy=4.0>]]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to stop gradient for some layers during backpropagation we can do this by using stop_gradient()\n",
    "def f(w1, w2):\n",
    "    # It will not calculate the derivative w.r.t (y)\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)  # 3x^2 + 2xy\n",
    "    #return 3 * w1 ** 2 + (2 * w1 * w2)  # 3x^2 + 2xy\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)  # same result as without stop_gradient()\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In some case our gradient tend to zeros for softplus or sigmoid function when input tends to zero or you do not change the weights\n",
    "x = tf.Variable(1e-50)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But calculating derivative or sigmoid is analytically feasible.\n",
    "tf.math.log(tf.exp(tf.constant(30., dtype=tf.float32)) + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "\n",
    "x = tf.Variable([1.0e30])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorate tensorflow custom_gradient function\n",
    "@tf.custom_gradient\n",
    "def my_softplus(z):\n",
    "    def my_softplus_gradients(grads):  # grads = backprop'ed from upper layers\n",
    "        return grads * (1 - 1 / (1 + tf.exp(z)))  # stable grads of softplus\n",
    "\n",
    "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "    return result, my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Custom Training Loops\n",
    "\n",
    "Wide & Deep paper uses two different optimizers: one for the wide path and the other for the deep path. Since the `fit()` method only uses one optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, build the simple model\n",
    "cls()\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "    tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\" for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end) # \\r is carriage return will ensure that the print statement execute on same line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(x_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn   = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics   = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "522/522 - mean: 3.9725 - mean_absolute_error: 0.9552\n",
      "Epoch 2/5\n",
      "522/522 - mean: 2.1014 - mean_absolute_error: 0.8969\n",
      "Epoch 3/5\n",
      "522/522 - mean: 1.5703 - mean_absolute_error: 0.8890\n",
      "Epoch 4/5\n",
      "522/522 - mean: 1.4374 - mean_absolute_error: 0.8994\n",
      "Epoch 5/5\n",
      "522/522 - mean: 1.3838 - mean_absolute_error: 0.9034\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(x_train_s, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses) # model loss basically regularization loss that we add in main loss\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # If you want to do any transformation with gradient you can do before the apply_gradients()\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        #if your model has variable constraints\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404d8409001d4c998af9ecbab60bb1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f970b6ada96e4c85b21d78cdaef56be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f151d8218dd54ccf891aa666496c2bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c569ca597874b30bc21fe838a178e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac1b11bee9749a9a7d361ce873cc188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0d5bd525f3461280908e5877afe6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Shows how to use the tqdm package to display nice progress bars\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict\n",
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(x_train_s, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "\n",
    "                steps.set_postfix(status)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
